{"./":{"url":"./","title":"Introduction","keywords":"","body":"Istio深度解析与项目实践 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/":{"url":"content/istio-introduction/","title":"微服务架构中的基础设施","keywords":"","body":"作为一种架构模式，微服务将复杂系统切分为数十乃至上百个小服务，每个服务负责实现一个独立的业务逻辑。这些小服务易于被小型的软件工程师团队所理解和修改，并带>来了语言和框架选择灵活性，缩短应用开发上线时间，可根据不同的工作负载和资源要求对服务进行独立缩扩容等优势。 另一方面，当应用被拆分为多个微服务进程后，进程内的方法调用变成了了进程间的远程调用。引入了对大量服务的连接、管理和监控的复杂性。 该变化带来了分布式系统的一系列问题，例如： 如何找到服务的提供方？ 如何保证远程方法调用的可靠性？ 如何保证服务调用的安全性？ 如何降低服务调用的延迟？ 如何进行端到端的调试？ 另外生产部署中的微服务实例也增加了运维的难度,例如： 如何收集大量微服务的性能指标已进行分析？ 如何在不影响上线业务的情况下对微服务进行升级？ 如何测试一个微服务集群部署的容错和稳定性？ 这些问题涉及到成百上千个服务的通信、管理、部署、版本、安全、故障转移、策略执行、遥测和监控等，要解决这些微服务架构引入的问题并非易事。 让我们来回顾一下微服务架构的发展过程。在出现服务网格之前，我们最开始在微服务应用程序内理服务之间的通讯逻辑，包括服务发现，熔断，重试，超时，加密，限流等>逻辑。 在一个分布式系统中，这部分逻辑比较复杂，为了为微服务应用提供一个稳定、可靠的基础设施层，避免大家重复造轮子，并减少犯错的可能，一般会通过对这部分负责服务>通讯的逻辑进行抽象和归纳，形成一个代码库供各个微服务应用程序使用，如下图所示： 公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题： 公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题： 微服务通讯逻辑对应用开发人员并不透明，应用开发人员需要理解并正确使用代码 库，不能将其全部精力聚焦于业务逻辑。 需要针对不同的语言/框架开发不同的代码库，反过来会影响微服务应用开发语言 和框架的选择，影响技术选择的灵活性。 随着时间的变化，代码库会存在不同的版本，不同版本代码库的兼容性和大量运行 环境中微服务的升级将成为一个难题。 可以将微服务之间的通讯基础设施层和TCP/IP协议栈进行类比。TCP/IP协议栈为操作系统中的所有应用提供基础通信服务，但TCP/IP协议栈和应用程序之间并没有紧密的耦合>关系，应用只需要使用TCP/IP协议提供的底层通讯功能,并不关心TCP/IP协议的实现，如IP如何进行路由，TCP如何创建链接等。 同样地，微服务应用也不应该需要关注服务发现，Load balancing，Retries，Circuit Breaker等微服务之间通信的底层细节。如果将为微服务提供通信服务的这部分逻辑从>应用程序进程中抽取出来，作为一个单独的进程进行部署，并将其作为服务间的通信代理，可以得到如下图所示的架构： 因为通讯代理进程伴随应用进程一起部署，因此形象地把这种部署方式称为“sidecar”/边车（即三轮摩托的挎斗）。 应用间的所有流量都需要经过代理，由于代理以sidecar方式和应用部署在同一台主机上，应用和代理之间的通讯可以被认为是可靠的。由代理来负责找到目的服务并负责通讯 的可靠性和安全等问题。 当服务大量部署时，随着服务部署的sidecar代理之间的连接形成了一个如下图所示的网格，该网格成为了微服务的通讯基础设施层，承载了微服务之间的所有流量，被称之为 Service Mesh（服务网格）。 _服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求可以在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由 一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但应用程序不需要知道它们的存在。 William Morgan WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? _ 服务网格中有数量众多的Sidecar代理，如果对每个代理分别进行设置，工作量将非常巨大。为了更方便地对服务网格中的代理进行统一集中控制，在服务网格上增加了控制面 组件。 这里我们可以类比SDN的概念，控制面就类似于SDN网管中的控制器，负责路由策略的指定和路由规则下发；数据面类似于SDN网络中交换机，负责数据包的转发。 由于微服务的所有通讯都由服务网格基础设施层提供，通过控制面板和数据面板的配合，可以对这些通讯进行监控、托管和控制，以实现微服务灰度发布，调用分布式追踪，>故障注入模拟测试，动态路由规则，微服务闭环控制等管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/微服务架构的演进.html":{"url":"content/istio-introduction/微服务架构的演进.html","title":"微服务架构的演进","keywords":"","body":"作为一种架构模式，微服务将复杂系统切分为数十乃至上百个小服务，每个服务负责实现一个独立的业务逻辑。这些小服务易于被小型的软件工程师团队所理解和修改，并带 来了语言和框架选择灵活性，缩短应用开发上线时间，可根据不同的工作负载和资源要求对服务进行独立缩扩容等优势。 另一方面，当应用被拆分为多个微服务进程后，进程内的方法调用变成了了进程间的远程调用。引入了对大量服务的连接、管理和监控的复杂性。 该变化带来了分布式系统的一系列问题，例如： 如何找到服务的提供方？ 如何保证远程方法调用的可靠性？ 如何保证服务调用的安全性？ 如何降低服务调用的延迟？ 如何进行端到端的调试？ 另外生产部署中的微服务实例也增加了运维的难度,例如： 如何收集大量微服务的性能指标已进行分析？ 如何在不影响上线业务的情况下对微服务进行升级？ 如何测试一个微服务集群部署的容错和稳定性？ 这些问题涉及到成百上千个服务的通信、管理、部署、版本、安全、故障转移、策略执行、遥测和监控等，要解决这些微服务架构引入的问题并非易事。 让我们来回顾一下微服务架构的发展过程。在出现服务网格之前，我们最开始在微服务应用程序内理服务之间的通讯逻辑，包括服务发现，熔断，重试，超时，加密，限流等逻辑。 在一个分布式系统中，这部分逻辑比较复杂，为了为微服务应用提供一个稳定、可靠的基础设施层，避免大家重复造轮子，并减少犯错的可能，一般会通过对这部分负责服务通讯的逻辑进行抽象和归纳，形成一个代码库供各个微服务应用程序使用，如下图所示： 公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题： 公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题： 微服务通讯逻辑对应用开发人员并不透明，应用开发人员需要理解并正确使用代码 库，不能将其全部精力聚焦于业务逻辑。 需要针对不同的语言/框架开发不同的代码库，反过来会影响微服务应用开发语言 和框架的选择，影响技术选择的灵活性。 随着时间的变化，代码库会存在不同的版本，不同版本代码库的兼容性和大量运行 环境中微服务的升级将成为一个难题。 可以将微服务之间的通讯基础设施层和TCP/IP协议栈进行类比。TCP/IP协议栈为操作系统中的所有应用提供基础通信服务，但TCP/IP协议栈和应用程序之间并没有紧密的耦合关系，应用只需要使用TCP/IP协议提供的底层通讯功能,并不关心TCP/IP协议的实现，如IP如何进行路由，TCP如何创建链接等。 同样地，微服务应用也不应该需要关注服务发现，Load balancing，Retries，Circuit Breaker等微服务之间通信的底层细节。如果将为微服务提供通信服务的这部分逻辑从应用程序进程中抽取出来，作为一个单独的进程进行部署，并将其作为服务间的通信代理，可以得到如下图所示的架构： 因为通讯代理进程伴随应用进程一起部署，因此形象地把这种部署方式称为“sidecar”/边车（即三轮摩托的挎斗）。 应用间的所有流量都需要经过代理，由于代理以sidecar方式和应用部署在同一台主机上，应用和代理之间的通讯可以被认为是可靠的。由代理来负责找到目的服务并负责通讯的可靠性和安全等问题。 当服务大量部署时，随着服务部署的sidecar代理之间的连接形成了一个如下图所示的网格，该网格成为了微服务的通讯基础设施层，承载了微服务之间的所有流量，被称之为Service Mesh（服务网格）。 _服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求可以在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由 一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但应用程序不需要知道它们的存在。 William Morgan WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? _ 服务网格中有数量众多的Sidecar代理，如果对每个代理分别进行设置，工作量将非常巨大。为了更方便地对服务网格中的代理进行统一集中控制，在服务网格上增加了控制面 组件。 这里我们可以类比SDN的概念，控制面就类似于SDN网管中的控制器，负责路由策略的指定和路由规则下发；数据面类似于SDN网络中交换机，负责数据包的转发。 由于微服务的所有通讯都由服务网格基础设施层提供，通过控制面板和数据面板的配合，可以对这些通讯进行监控、托管和控制，以实现微服务灰度发布，调用分布式追踪，故障注入模拟测试，动态路由规则，微服务闭环控制等管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/Istio服务网格.html":{"url":"content/istio-introduction/Istio服务网格.html","title":"Istio服务网格","keywords":"","body":"Istio是一个Service Mesh开源项目，是Google继Kubernetes之后的又一力作，主要参与的公司包括Google，IBM和Lyft。 凭借kubernetes良好的架构设计及其强大的扩展性，Google围绕kubernetes打造一个生态系统。Kubernetes用于微服务的编排（编排是英文Orchestration的直译，用大白话说就是描述一组微服务之间的关联关系，并负责微服务的部署、终止、升级、缩扩容等）。其向下用CNI(容器网络接口），CRI（容器运行时接口）标准接口可以对接不同的网络和容器运行时实现，提供微服务运行的基础设施。向上则用Istio提供了微服务治理功能。 由下图可见，Istio补充了Kubernetes生态圈的重要一环，是Google的微服务版图里一个里程碑式的扩张。 Google借Istio的力量推动微服务治理的事实标准，对Google自身的产品Google Cloud有极其重大的意义。其他的云服务厂商，如Redhat，Pivotal，Nginx，Buoyant等看到大势所趋，也纷纷跟进，宣布自身产品和Istio进行集成，以避免自己被落下，丢失其中的市场机会。 可以预见不久的将来，对于云原生应用而言，采用kubernetes进行服务部署和集群管理，采用Istio处理服务通讯和治理，将成为微服务应用的标准配置。 Istio服务包括网格由数据面和控制面两部分。 数据面由一组智能代理（Envoy）组成，代理部署为边车，调解和控制微服务之间所有的网络通信。 控制面负责管理和配置代理来路由流量，以及在运行时执行策略。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/Istio控制面.html":{"url":"content/istio-introduction/Istio控制面.html","title":"Istio控制面","keywords":"","body":"Istio控制面板包括3个组件:Pilot, Mixer和Istio-Auth。 Pilot Pilot维护了网格中的服务的标准模型，这个标准模型是独立于各种底层平台的。Pilot通过适配器和各底层平台对接，以填充此标准模型。 例如Pilot中的Kubernetes适配器通过Kubernetes API服务器得到kubernetes中pod注册信息的更改，入口资源以及存储流量管理规则等信息，然后将该数据被翻译为标准模型提供给Pilot使用。通过适配器模式，Pilot还可以从Mesos, Cloud Foundry, Consul中获取服务信息，也可以开发适配器将其他提供服务发现的组件集成到Pilot中。 除此以外，Pilo还定义了一套和数据面通信的标准API，API提供的接口内容包括服务发现 、负载均衡池和路由表的动态更新。通过该标准API将控制面和数据面进行了解耦，简化了设计并提升了跨平台的可移植性。基于该标准API已经实现了多种Sidecar代理和Istio的集成，除Istio目前集成的Envoy外，还可以和Linkerd, Nginmesh等第三方通信代理进行集成，也可以基于该API自己编写Sidecar实现。 Pilot还定义了一套DSL（Domain Specific Language）语言，DSL语言提供了面向业务的高层抽象，可以被运维人员理解和使用。运维人员使用该DSL定义流量规则并下发到Pilot，这些规则被Pilot翻译成数据面的配置，再通过标准API分发到Envoy实例，可以在运行期对微服务的流量进行控制和调整。 Mixer 在微服务应用中，通常需要部署一些基础的后端公共服务以用于支撑业务功能。这些基础设施包括策略类如访问控制，配额管理；以及遥测报告如APM，日志等。微服务应用和这些后端支撑系统之间一般是直接集成的，这导致了应用和基础设置之间的紧密耦合，如果因为运维原因需要对基础设置进行升级或者改动，则需要修改各个微服务的应用代码，反之亦然。 为了解决该问题，Mixer在应用程序代码和基础架构后端之间引入了一个通用中间层。该中间层解耦了应用和后端基础设施，应用程序代码不再将应用程序代码与特定后端集成在一起，而是与Mixer进行相当简单的集成，然后Mixer负责与后端系统连接。 Mixer主要提供了三个核心功能： 前提条件检查。允许服务在响应来自服务消费者的传入请求之前验证一些前提条件。前提条件可以包括服务使用者是否被正确认证，是否在服务的白名单上，是否通过ACL检查等等。 配额管理。 使服务能够在分配和释放多个维度上的配额，配额这一简单的资源管理工具可以在服务消费者对有限资源发生争用时，提供相对公平的（竞争手段）。Rate Limiting就是配额的一个例子。 遥测报告。使服务能够上报日志和监控。在未来，它还将启用针对服务运营商以及服务消费者的跟踪和计费流。 Mixer的架构如图所示: 首先，Sidecar会从每一次请求中收集相关信息，如请求的路径，时间，源IP，目地服务，tracing头，日志等，并请这些属性上报给Mixer。Mixer和后端服务之间是通过适配器进行连接的，Mixer将Sidecar上报的内容通过适配器发送给后端服务。 由于Sidecar只和Mixer进行对接，和后端服务之间并没有耦合，因此使用Mixer适配器机制可以接入不同的后端服务，而不需要修改应用的代码，例如通过不同的Mixer适配器，可以把Metrics收集到Prometheus或者InfluxDB，甚至可以在不停止应用服务的情况下动态切换后台服务。 其次，Sidecar在进行每次请求处理时会通过Mixer进行策略判断，并根据Mixer返回的结果决定是否继续处理该次调用。通过该方式，Mixer将策略决策移出应用层，使运维人员可以在运行期对策略进行配置，动态控制应用的行为，提高了策略控制的灵活性。例如可以配置每个微服务应用的访问白名单，不同客户端的Rate limiting，等等。 逻辑上微服务之间的每一次请求调用都会经过两次Mixer的处理：调用前进行策略判断，调用后进行遥测数据收集。Istio采用了一些机制来避免Mixer的处理影响Envoy的转发效率。 从上图可以看到，Istio在Envoy中增加了一个Mixer Filter，该Filter和控制面的Mixer组件进行通讯，完成策略控制和遥测数据收集功能。Mixer Filter中保存有策略判断所需的数据缓存，因此大部分策略判断在Envoy中就处理了，不需要发送请求到Mixer。另外Envoy收集到的遥测数据会先保存在Envoy的缓存中，每隔一段时间再通过批量的方式上报到Mixer。 Auth Istio支持双向SSL认证（Mutual SSL Authentication）和基于角色的访问控制（RBAC），以提供端到端的安全解决方案。 认证 Istio提供了一个内部的CA(证书机构),该CA为每个服务颁发证书，提供服务间访问的双向SSL身份认证，并进行通信加密，其架构如下图所示： 其工作机制如下： 部署时： CA监听Kubernetes API Server, 为集群中的每一个Service Account创建一对密钥和证书，并发送给Kubernetes API Server。注意这里不是为每个服务生成一个证书，而是为每个Service Account生成一个证书。Service Account和kubernetes中部署的服务可以是一对多的关系。Service Account被保存在证书的SAN(Subject Alternative Name)字段中。 当Pod创建时，Kubernetes根据该Pod关联的Service Account将密钥和证书以Kubernetes Secrets资源的方式加载为Pod的Volume，以供Envoy使用。 Pilot生成数据面的配置，包括Envoy需使用的密钥和证书信息，以及哪个Service Account可以允许运行哪些服务，下发到Envoy。 备注：如果是虚机环境，则采用一个Node Agent生成密钥，向Istio CA申请证书，然后将证书传递给Envoy。 运行时： 服务客户端的出站请求被Envoy接管。 客户端的Envoy和服务端的Envoy开始双向SSL握手。在握手阶段，客户端Envoy会验证服务端Envoy证书中的Service Account有没有权限运行该请求的服务，如没有权限，则认为服务端不可信，不能创建链接。 当加密TSL链接创建好后，请求数据被发送到服务端的Envoy，然后被Envoy通过一个本地的TCP链接发送到服务中。 鉴权 Istio“基于角色的访问控制”（RBAC）提供了命名空间，服务，方法三个不同大小粒度的服务访问权限控制。其架构如下图所示： 管理人员可以定制访问控制的安全策略，这些安全策略保存在Istio Config Store中。 Istio RBAC Engine从Config Store中获取安全策略，根据安全策略对客户端发起的请求进行判断，并返回鉴权结果（允许或者禁止）。 Istio RBAC Engine目前被实现为一个Mixer Adapter，因此其可以从Mixer传递过来的上下文中获取到访问请求者的身份（Subject）和操作请求（Action），并通过Mixer对访问请求进行策略控制，允许或者禁止某一次请求。 Istio Policy中包含两个基本概念： ServiceRole，定义一个角色，并为该角色指定对网格中服务的访问权限。指定角色访问权限时可以在命名空间，服务，方法的不同粒度进行设置。 ServiceRoleBinding，将角色绑定到一个Subject，可以是一个用户，一组用户或者一个服务。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/Istio数据面.html":{"url":"content/istio-introduction/Istio数据面.html","title":"Istio数据面","keywords":"","body":"Istio数据面以“边车”(sidecar)的方式和微服务一起部署，为微服务提供安全、快速、可靠的服务间通讯。由于Istio的控制面和数据面以标准接口进行交互，因此数据可以有多种实现，Istio缺省使用了Envoy代理的扩展版本。 Envoy是以C ++开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy的许多内置功能被Istio发扬光大，例如动态服务发现，负载均衡，TLS加密，HTTP/2 & gRPC代理，熔断器，路由规则，故障注入和遥测等。 Istio数据面支持的特性如下： Outbound特性 Inbound特性 Service authentication（服务认证） Service authentication（服务认证） Load Balancing（负载均衡） Authorization（鉴权） Retry and circuit breaker（重试和断路器） Rate limits（请求限流） Fine-grained routing（细粒度的路由） Load shedding（负载控制） Telemetry（遥测） Telemetry（遥测） Request Tracing（分布式追踪） Request Tracing（分布式追踪） Fault Injection（故障注入） Fault Injection（故障注入） 备注：Outbound特性是指服务请求侧的Sidecar提供的功能特性，而Inbound特性是指服务提供侧Sidecar提供的功能特性。一些特性如遥测和分布式跟踪需要两侧的Sidecar都提供支持；而另一些特性则只需要在一侧提供，例如鉴权只需要在服务提供侧提供，重试只需要在请求侧提供。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/应用场景-分布式调用跟踪.html":{"url":"content/istio-introduction/应用场景-分布式调用跟踪.html","title":"应用场景-分布式调用跟踪","keywords":"","body":"在微服务架构中，业务的调用链非常复杂，一个来自用户的请求可能涉及到几十个服务的协同处理。因此需要一个跟踪系统来记录和分析同一次请求在整个调用链上的相关事件，从而帮助研发和运维人员分析系统瓶颈，快速定位异常和优化调用链路。 Istio通过在Envoy代理上收集调用相关数据，实现了对应用无侵入的分布式调用跟踪分析。 Istio实现分布式调用追踪的原理如下图所示: Envoy收集一个端到端调用中的各个分段的数据，并将这些调用追踪信息发送给Mixer，Mixer Adapter 将追踪信息发送给相应的服务后端进行处理。整个调用追踪信息的生成流程不需要应用程序介入，因此不需要将分布式跟踪相关代码注入到应用程序中。 注意：应用仍需要在进行出口调用时将收到的入口请求中tracing相关的header转发出去，传递给调用链中下一个边车进行处理。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/应用场景-度量收集.html":{"url":"content/istio-introduction/应用场景-度量收集.html","title":"应用场景-度量收集","keywords":"","body":"Istio 实现度量收集的原理如下图所示: Envoy收集指标相关的原始数据，如请求的服务，HTTP状态码，调用时延等，这些收集到的指标数据被送到Mixer，通过Mixer Adapters 将指标信息转换后发送到后端的监控系统中。由于Mixer使用了插件机制，后端监控系统可以根据需要在运行期进行动态切换。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/应用场景-灰度发布.html":{"url":"content/istio-introduction/应用场景-灰度发布.html","title":"应用场景-灰度发布","keywords":"","body":"当应用上线以后，运维面临的一大挑战是如何能够在不影响已上线业务的情况下进行升级。无论进行了多么完善的测试，都无法保证线下测试时发现所有潜在故障。在无法百分百避免版本升级故障的情况下，需要通过一种方式进行可控的版本发布，把故障影响控制在可以接受的范围内，并可以快速回退。 可以通过灰度发布（又名金丝雀发布）来实现业务从老版本到新版本的平滑过渡，并避免升级过程中出现的问题对用户造成的影响。 Istio通过高度的抽象和良好的设计采用一致的方式实现了灰度发布。在发布新版本后，运维人员可以通过定制路由规则将特定的流量（如具有指定特征的测试用户）导入新版本服务中以进行测试。通过渐进受控地向新版本导入生产流量，可以最小化升级中出现的故障对用户的影响。 采用Istio进行灰度发布的流程如下图所示： 首先，通过部署新版本的服务，并将通过路由规则将金丝雀用户的流量导入到新版本服务中 测试稳定后，使用路由规则将生产流量逐渐导入到新版本系统中，如按5%，10%，50%，80%逐渐导入。 如果新版本工作正常，则最后将所有流量导入到新版本服务中，并将老版本服务下线；如中间出现问题，则可以将流量重新导回老版本，在新版本中修复故障后采用该流程重新发布。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/应用场景-断路器.html":{"url":"content/istio-introduction/应用场景-断路器.html","title":"应用场景-断路器","keywords":"","body":"在微服务架构中，存在着许许多多的服务单元，若一个服务出现故障，就会因依赖关系形成故障蔓延，最终导致整个系统的瘫痪，这样的架构相较传统架构就更加的不稳定。为了解决这样的问题，因此产生了断路器模式。 断路器模式指，在某个服务发生故障时，断路器的故障监控向调用放返回一个及时的错误响应，而不是长时间的等待。这样就不会使得调用线程因调用故障被长时间占用，从而避免了故障在整个系统中的蔓延。 Istio 实现断路器的原理如下图所示: 管理员通过destination policy设置断路触发条件，断路时间等参数。例如设置服务B发生10次5XX错误后断路15分钟。则当服务B的某一实例满足断路条件后，就会被从LB池中移除15分钟。在这段时间内，Envoy将不再把客户端的请求转发到该服务实例。 Istio的断路器还支持配置最大链接数，最大待处理请求数，最大请求数，每链接最大请求数，重试次数等参数。当达到设置的最大请求数后，新发起的请求会被Envoy直接拒绝。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/istio-introduction/应用场景-故障注入.html":{"url":"content/istio-introduction/应用场景-故障注入.html","title":"应用场景-故障注入","keywords":"","body":"对于一个大型微服务应用而言，系统的健壮性非常重要。在微服务系统中存在大量的服务实例，当部分服务实例出现问题时，微服务应用需要具有较高的容错性，通过重试，断路，自愈等手段保证系统能够继续对外正常提供服务。因此在应用发布到生产系统强需要对系统进行充分的健壮性测试。 对微服务应用进行健壮性测试的一个最大的困难是如何对系统故障进行模拟。在一个部署了成百上千微服务的测试环境中，如果想通过对应用，主机或者交换机进行设置来模拟微服务之间的通信故障是非常困难的。 Istio通过服务网格承载了微服务之间的通信流量，因此可以在网格中通过规则进行故障注入，模拟部分微服务出现故障的情况，对整个应用的健壮性进行测试。 故障注入的原理如下图所示： 测试人员通过Pilot向Envoy注入了一个规则，为发向服务MS-B的请求加入了指定时间的延迟。当客户端请求发向MSB-B时，Envoy会根据该规则为该请求加入时延，引起客户的请求超时。通过设置规则注入故障的方式，测试人员可以很方便地模拟微服务之间的各种通信故障，对微服务应用的健壮性进行较为完整的模拟测试。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/traffic-management/":{"url":"content/traffic-management/","title":"Istio流量管理实现机制","keywords":"","body":"前言 Istio作为一个service mesh开源项目,其中最重要的功能就是对网格中微服务之间的流量进行管理,包括服务发现,请求路由和服务间的可靠通信。Istio实现了service mesh的控制面，并整合Envoy开源项目作为数据面的sidecar，一起对流量进行控制。 Istio体系中流量管理配置下发以及流量规则如何在数据面生效的机制相对比较复杂，通过官方文档容易管中窥豹，难以了解其实现原理。本文尝试结合系统架构、配置文件和代码对Istio流量管理的架构和实现机制进行分析，以达到从整体上理解Pilot和Envoy的流量管理机制的目的。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 22:40:14 "},"content/traffic-management/architecture.html":{"url":"content/traffic-management/architecture.html","title":"Istio流量管理高层架构","keywords":"","body":"Pilot高层架构 Istio控制面中负责流量管理的组件为Pilot，Pilot的高层架构如下图所示： Pilot Architecture（来自[Isio官网文档](https://istio.io/docs/concepts/traffic-management/)[[1]](#ref01)) 根据上图,Pilot主要实现了下述功能： 统一的服务模型 Pilot定义了网格中服务的标准模型，这个标准模型独立于各种底层平台。由于有了该标准模型，各个不同的平台可以通过适配器和Pilot对接，将自己特有的服务数据格式转换为标准格式，填充到Pilot的标准模型中。 例如Pilot中的Kubernetes适配器通过Kubernetes API服务器得到kubernetes中service和pod的相关信息，然后翻译为标准模型提供给Pilot使用。通过适配器模式，Pilot还可以从Mesos, Cloud Foundry, Consul等平台中获取服务信息，还可以开发适配器将其他提供服务发现的组件集成到Pilot中。 标准数据面 API Pilo使用了一套起源于Envoy项目的标准数据面API[2]来将服务信息和流量规则下发到数据面的sidecar中。 通过采用该标准API，Istio将控制面和数据面进行了解耦，为多种数据面sidecar实现提供了可能性。事实上基于该标准API已经实现了多种Sidecar代理和Istio的集成，除Istio目前集成的Envoy外，还可以和Linkerd, Nginmesh等第三方通信代理进行集成，也可以基于该API自己编写Sidecar实现。 控制面和数据面解耦是Istio后来居上，风头超过Service mesh鼻祖Linkerd的一招妙棋。Istio站在了控制面的高度上，而Linkerd则成为了可选的一种sidecar实现，可谓降维打击的一个典型成功案例！ 数据面标准API也有利于生态圈的建立，开源，商业的各种sidecar以后可能百花齐放，用户也可以根据自己的业务场景选择不同的sidecar和控制面集成，如高吞吐量的，低延迟的，高安全性的等等。有实力的大厂商可以根据该API定制自己的sidecar，例如蚂蚁金服开源的Golang版本的Sidecar MOSN(Modular Observable Smart Netstub)（SOFAMesh中Golang版本的Sidecar)；小厂商则可以考虑采用成熟的开源项目或者提供服务的商业sidecar实现。 备注：Istio和Envoy项目联合制定了Envoy V2 API,并采用该API作为Istio控制面和数据面流量管理的标准接口。 业务DSL语言 Pilot还定义了一套DSL（Domain Specific Language）语言，DSL语言提供了面向业务的高层抽象，可以被运维人员理解和使用。运维人员使用该DSL定义流量规则并下发到Pilot，这些规则被Pilot翻译成数据面的配置，再通过标准API分发到Envoy实例，可以在运行期对微服务的流量进行控制和调整。 Pilot的规则DSL是采用K8S API Server中的Custom Resource (CRD)[3]实现的，因此和其他资源类型如Service Pod Deployment的创建和使用方法类似，都可以用Kubectl进行创建。 通过运用不同的流量规则，可以对网格中微服务进行精细化的流量控制，如按版本分流，断路器，故障注入，灰度发布等。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 22:41:07 "},"content/traffic-management/component.html":{"url":"content/traffic-management/component.html","title":"Istio流量管理相关组件","keywords":"","body":"Istio流量管理相关组件 我们可以通过下图了解Istio流量管理涉及到的相关组件。虽然该图来自Istio Github old pilot repo, 但图中描述的组件及流程和目前Pilot的最新代码的架构基本是一致的。 Pilot Design Overview (来自[Istio old_pilot_repo](https://github.com/istio/old_pilot_repo/blob/master/doc/design.md)[[4]](#ref04)) 图例说明：图中红色的线表示控制流，黑色的线表示数据流。蓝色部分为和Pilot相关的组件。 从上图可以看到，Istio中和流量管理相关的有以下组件： 控制面组件 Discovery Services 对应的docker为gcr.io/istio-release/pilot,进程为pilot-discovery，该组件的功能包括： 从Service provider（如kubernetes或者consul）中获取服务信息 从K8S API Server中获取流量规则(K8S CRD Resource) 将服务信息和流量规则转化为数据面可以理解的格式，通过标准的数据面API下发到网格中的各个sidecar中。 K8S API Server 提供Pilot相关的CRD Resource的增、删、改、查。和Pilot相关的CRD有以下几种: Virtualservice：用于定义路由规则，如根据来源或 Header 制定规则，或在不同服务版本之间分拆流量。 DestinationRule：定义目的服务的配置策略以及可路由子集。策略包括断路器、负载均衡以及 TLS 等。 ServiceEntry：用 ServiceEntry 可以向Istio中加入附加的服务条目，以使网格内可以向istio 服务网格之外的服务发出请求。 Gateway：为网格配置网关，以允许一个服务可以被网格外部访问。 EnvoyFilter：可以为Envoy配置过滤器。由于Envoy已经支持Lua过滤器，因此可以通过EnvoyFilter启用Lua过滤器，动态改变Envoy的过滤链行为。我之前一直在考虑如何才能动态扩展Envoy的能力，EnvoyFilter提供了很灵活的扩展性。 数据面组件 在数据面有两个进程Pilot-agent和envoy，这两个进程被放在一个docker容器gcr.io/istio-release/proxyv2中。 Pilot-agent 该进程根据K8S API Server中的配置信息生成Envoy的配置文件，并负责启动Envoy进程。注意Envoy的大部分配置信息都是通过xDS接口从Pilot中动态获取的，因此Agent生成的只是用于初始化Envoy的少量静态配置。在后面的章节中，本文将对Agent生成的Envoy配置文件进行进一步分析。 Envoy Envoy由Pilot-agent进程启动，启动后，Envoy读取Pilot-agent为它生成的配置文件，然后根据该文件的配置获取到Pilot的地址，通过数据面标准API的xDS接口从pilot拉取动态配置信息，包括路由（route），监听器（listener），服务集群（cluster）和服务端点（endpoint）。Envoy初始化完成后，就根据这些配置信息对微服务间的通信进行寻址和路由。 命令行工具 kubectl和Istioctl，由于Istio的配置是基于K8S的CRD，因此可以直接采用kubectl对这些资源进行操作。Istioctl则针对Istio对CRD的操作进行了一些封装。Istioctl支持的功能参见该表格。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 22:41:55 "},"content/traffic-management/data-plane-api.html":{"url":"content/traffic-management/data-plane-api.html","title":"数据面标准接口","keywords":"","body":"数据面标准API 前面讲到，Pilot采用了一套标准的API来向数据面Sidecar提供服务发现，负载均衡池和路由表等流量管理的配置信息。该标准API的文档参见Envoy v2 API[5]。Data Plane API Protocol Buffer Definition[6])给出了v2 grpc接口相关的数据结构和接口定义。 （备注：Istio早期采用了Envoy v1 API，目前的版本中则使用V2 API，V1已被废弃）。 基本概念和术语 首先我们需要了解数据面API中涉及到的一些基本概念： Host：能够进行网络通信的实体（如移动设备、服务器上的应用程序）。在此文档中，主机是逻辑网络应用程序。一块物理硬件上可能运行有多个主机，只要它们是可以独立寻址的。在EDS接口中，也使用“Endpoint”来表示一个应用实例，对应一个IP+Port的组合。 Downstream：下游主机连接到 Envoy，发送请求并接收响应。 Upstream：上游主机接收来自 Envoy 的连接和请求，并返回响应。 Listener：监听器是命名网地址（例如，端口、unix domain socket等)，可以被下游客户端连接。Envoy 暴露一个或者多个监听器给下游主机连接。在Envoy中,Listener可以绑定到端口上直接对外服务，也可以不绑定到端口上，而是接收其他listener转发的请求。 Cluster：集群是指 Envoy 连接到的逻辑上相同的一组上游主机。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到哪个集群成员。 XDS服务接口 Istio数据面API定义了xDS服务接口，Pilot通过该接口向数据面sidecar下发动态配置信息，以对Mesh中的数据流量进行控制。xDS中的DS表示discovery service，即发现服务，表示xDS接口使用动态发现的方式提供数据面所需的配置数据。而x则是一个代词，表示有多种discover service。这些发现服务及对应的数据结构如下： LDS (Listener Discovery Service) envoy.api.v2.Listener CDS (Cluster Discovery Service) envoy.api.v2.RouteConfiguration EDS (Endpoint Discovery Service) envoy.api.v2.Cluster RDS (Route Discovery Service) envoy.api.v2.ClusterLoadAssignment XDS服务接口的最终一致性考虑 xDS的几个接口是相互独立的，接口下发的配置数据是最终一致的。但在配置更新过程中，可能暂时出现各个接口的数据不匹配的情况，从而导致部分流量在更新过程中丢失。 设想这种场景：在CDS/EDS只知道cluster X的情况下,RDS的一条路由配置将指向Cluster X的流量调整到了Cluster Y。在CDS/EDS向Mesh中Envoy提供Cluster Y的更新前，这部分导向Cluster Y的流量将会因为Envoy不知道Cluster Y的信息而被丢弃。 对于某些应用来说，短暂的部分流量丢失是可以接受的，例如客户端重试可以解决该问题，并不影响业务逻辑。对于另一些场景来说，这种情况可能无法容忍。可以通过调整xDS接口的更新逻辑来避免该问题，对上面的情况，可以先通过CDS/EDS更新Y Cluster，然后再通过RDS将X的流量路由到Y。 一般来说，为了避免Envoy配置数据更新过程中出现流量丢失的情况，xDS接口应采用下面的顺序： CDS 首先更新Cluster数据（如果有变化） EDS 更新相应Cluster的Endpoint信息（如果有变化） LDS 更新CDS/EDS相应的Listener。 RDS 最后更新新增Listener相关的Route配置。 删除不再使用的CDS cluster和 EDS endpoints。 ADS聚合发现服务 保证控制面下发数据一致性，避免流量在配置更新过程中丢失的另一个方式是使用ADS(Aggregated Discovery Services)，即聚合的发现服务。ADS通过一个gRPC流来发布所有的配置更新，以保证各个xDS接口的调用顺序，避免由于xDS接口更新顺序导致的配置数据不一致问题。 关于XDS接口的详细介绍可参考xDS REST and gRPC protocol[7] Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 22:42:23 "},"content/traffic-management/bookinfo.html":{"url":"content/traffic-management/bookinfo.html","title":"对Bookinfo进行端到端分析","keywords":"","body":"Bookinfo 示例程序分析 下面我们以Bookinfo为例对Istio中的流量管理实现机制，以及控制面和数据面的交互进行进一步分析。 Bookinfo程序结构 下图显示了Bookinfo示例程序中各个组件的IP地址，端口和调用关系，以用于后续的分析。 xDS接口调试方法 首先我们看看如何对xDS接口的相关数据进行查看和分析。Envoy v2接口采用了gRPC，由于gRPC是基于二进制的RPC协议，无法像V1的REST接口一样通过curl和浏览器进行进行分析。但我们还是可以通过Pilot和Envoy的调试接口查看xDS接口的相关数据。 Pilot调试方法 Pilot在9093端口提供了下述调试接口[8]下述方法查看xDS接口相关数据。 PILOT=istio-pilot.istio-system:9093 # What is sent to envoy # Listeners and routes curl $PILOT/debug/adsz # Endpoints curl $PILOT/debug/edsz # Clusters curl $PILOT/debug/cdsz Envoy调试方法 Envoy提供了管理接口，缺省为localhost的15000端口，可以获取listener，cluster以及完整的配置数据导出功能。 kubectl exec productpage-v1-54b8b9f55-bx2dq -c istio-proxy curl http://127.0.0.1:15000/help /: Admin home page /certs: print certs on machine /clusters: upstream cluster status /config_dump: dump current Envoy configs (experimental) /cpuprofiler: enable/disable the CPU profiler /healthcheck/fail: cause the server to fail health checks /healthcheck/ok: cause the server to pass health checks /help: print out list of admin commands /hot_restart_version: print the hot restart compatibility version /listeners: print listener addresses /logging: query/change logging levels /quitquitquit: exit the server /reset_counters: reset all counters to zero /runtime: print runtime values /runtime_modify: modify runtime values /server_info: print server version/status information /stats: print server stats /stats/prometheus: print server stats in prometheus format 进入productpage pod 中的istio-proxy(Envoy) container，可以看到有下面的监听端口 9080: productpage进程对外提供的服务端口 15001: Envoy的入口监听器，iptable会将pod的流量导入该端口中由Envoy进行处理 15000: Envoy管理端口，该端口绑定在本地环回地址上，只能在Pod内访问。 kubectl exec t productpage-v1-54b8b9f55-bx2dq -c istio-proxy -- netstat -ln Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:9080 0.0.0.0:* LISTEN - tcp 0 0 127.0.0.1:15000 0.0.0.0:* LISTEN 13/envoy tcp 0 0 0.0.0.0:15001 0.0.0.0:* LISTEN 13/envoy Envoy启动过程分析 Istio通过K8s的Admission webhook[9]机制实现了sidecar的自动注入，Mesh中的每个微服务会被加入Envoy相关的容器。下面是Productpage微服务的Pod内容，可见除productpage之外，Istio还在该Pod中注入了两个容器gcr.io/istio-release/proxy_init和gcr.io/istio-release/proxyv2。 备注：下面Pod description中只保留了需要关注的内容，删除了其它不重要的部分。为方便查看，本文中后续的其它配置文件以及命令行输出也会进行类似处理。 ubuntu@envoy-test:~$ kubectl describe pod productpage-v1-54b8b9f55-bx2dq Name: productpage-v1-54b8b9f55-bx2dq Namespace: default Init Containers: istio-init: Image: gcr.io/istio-release/proxy_init:1.0.0 Args: -p 15001 -u 1337 -m REDIRECT -i * -x -b 9080, -d Containers: productpage: Image: istio/examples-bookinfo-productpage-v1:1.8.0 Port: 9080/TCP istio-proxy: Image: gcr.io/istio-release/proxyv2:1.0.0 Args: proxy sidecar --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15007 --discoveryRefreshDelay 1s --zipkinAddress zipkin.istio-system:9411 --connectTimeout 10s --statsdUdpAddress istio-statsd-prom-bridge.istio-system:9125 --proxyAdminPort 15000 --controlPlaneAuthPolicy NONE Proxy_init Productpage的Pod中有一个InitContainer proxy_init，InitContrainer是K8S提供的机制，用于在Pod中执行一些初始化任务.在Initialcontainer执行完毕并退出后，才会启动Pod中的其它container。 我们看一下proxy_init容器中的内容： ubuntu@envoy-test:~$ sudo docker inspect gcr.io/istio-release/proxy_init:1.0.0 [ { \"RepoTags\": [ \"gcr.io/istio-release/proxy_init:1.0.0\" ], \"ContainerConfig\": { \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/sh\", \"-c\", \"#(nop) \", \"ENTRYPOINT [\\\"/usr/local/bin/istio-iptables.sh\\\"]\" ], \"Entrypoint\": [ \"/usr/local/bin/istio-iptables.sh\" ], }, } ] 从上面的命令行输出可以看到，Proxy_init中执行的命令是istio-iptables.sh，该脚本源码较长，就不列出来了，有兴趣可以在Istio 源码仓库的tools/deb/istio-iptables.sh查看。 该脚本的作用是通过配置iptable来劫持Pod中的流量。结合前面Pod中该容器的命令行参数-p 15001，可以得知Pod中的数据流量被iptable拦截，并发向Envoy的15001端口。 -u 1337参数用于排除用户ID为1337，即Envoy自身的流量，以避免Iptable把Envoy发出的数据又重定向到Envoy，形成死循环。 Proxyv2 前面提到，该容器中有两个进程Pilot-agent和envoy。我们进入容器中看看这两个进程的相关信息。 ubuntu@envoy-test:~$ kubectl exec productpage-v1-54b8b9f55-bx2dq -c istio-proxy -- ps -ef UID PID PPID C STIME TTY TIME CMD istio-p+ 1 0 0 Sep06 ? 00:00:00 /usr/local/bin/pilot-agent proxy sidecar --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15007 --discoveryRefreshDelay 1s --zipkinAddress zipkin.istio-system:9411 --connectTimeout 10s --statsdUdpAddress istio-statsd-prom-bridge.istio-system:9125 --proxyAdminPort 15000 --controlPlaneAuthPolicy NONE istio-p+ 13 1 0 Sep06 ? 00:47:37 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage --service-node sidecar~192.168.206.23~productpage-v1-54b8b9f55-bx2dq.default~default.svc.cluster.local --max-obj-name-len 189 -l warn --v2-config-only Envoy的大部分配置都是dynamic resource，包括网格中服务相关的service cluster, listener, route规则等。这些dynamic resource是通过xDS接口从Istio控制面中动态获取的。但Envoy如何知道xDS server的地址呢？这是在Envoy初始化配置文件中以static resource的方式配置的。 Envoy初始配置文件 Pilot-agent进程根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件，并负责启动Envoy进程。从ps命令输出可以看到Pilot-agent在启动Envoy进程时传入了pilot地址和zipkin地址，并为Envoy生成了一个初始化配置文件envoy-rev0.json Pilot agent生成初始化配置文件的代码： https://github.com/istio/istio/blob/release-1.0/pkg/bootstrap/bootstrap_config.go 137行 // WriteBootstrap generates an envoy config based on config and epoch, and returns the filename. // TODO: in v2 some of the LDS ports (port, http_port) should be configured in the bootstrap. func WriteBootstrap(config *meshconfig.ProxyConfig, node string, epoch int, pilotSAN []string, opts map[string]interface{}) (string, error) { if opts == nil { opts = map[string]interface{}{} } if err := os.MkdirAll(config.ConfigPath, 0700); err != nil { return \"\", err } // attempt to write file fname := configFile(config.ConfigPath, epoch) cfg := config.CustomConfigFile if cfg == \"\" { cfg = config.ProxyBootstrapTemplatePath } if cfg == \"\" { cfg = DefaultCfgDir } ...... if config.StatsdUdpAddress != \"\" { h, p, err = GetHostPort(\"statsd UDP\", config.StatsdUdpAddress) if err != nil { return \"\", err } StoreHostPort(h, p, \"statsd\", opts) } fout, err := os.Create(fname) if err != nil { return \"\", err } // Execute needs some sort of io.Writer err = t.Execute(fout, opts) return fname, err } 可以使用下面的命令将productpage pod中该文件导出来查看其中的内容： kubectl exec productpage-v1-54b8b9f55-bx2dq -c istio-proxy -- cat /etc/istio/proxy/envoy-rev0.json > envoy-rev0.json 配置文件的结构如图所示： 其中各个配置节点的内容如下： Node 包含了Envoy所在节点相关信息。 \"node\": { \"id\": \"sidecar~192.168.206.23~productpage-v1-54b8b9f55-bx2dq.default~default.svc.cluster.local\", //用于标识envoy所代理的node（在k8s中对应为Pod）上的service cluster，来自于Envoy进程启动时的service-cluster参数 \"cluster\": \"productpage\", \"metadata\": { \"INTERCEPTION_MODE\": \"REDIRECT\", \"ISTIO_PROXY_SHA\": \"istio-proxy:6166ae7ebac7f630206b2fe4e6767516bf198313\", \"ISTIO_PROXY_VERSION\": \"1.0.0\", \"ISTIO_VERSION\": \"1.0.0\", \"POD_NAME\": \"productpage-v1-54b8b9f55-bx2dq\", \"istio\": \"sidecar\" } } Admin 配置Envoy的日志路径以及管理端口。 \"admin\": { \"access_log_path\": \"/dev/stdout\", \"address\": { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 15000 } } } Dynamic_resources 配置动态资源,这里配置了ADS服务器。 \"dynamic_resources\": { \"lds_config\": { \"ads\": {} }, \"cds_config\": { \"ads\": {} }, \"ads_config\": { \"api_type\": \"GRPC\", \"refresh_delay\": {\"seconds\": 1, \"nanos\": 0}, \"grpc_services\": [ { \"envoy_grpc\": { \"cluster_name\": \"xds-grpc\" } } ] } } ##### Static_resources 配置静态资源，包括了xds-grpc和zipkin两个cluster。其中xds-grpc cluster对应前面dynamic_resources中ADS配置，指明了Envoy用于获取动态资源的服务器地址。 \"static_resources\": { \"clusters\": [ { \"name\": \"xds-grpc\", \"type\": \"STRICT_DNS\", \"connect_timeout\": {\"seconds\": 10, \"nanos\": 0}, \"lb_policy\": \"ROUND_ROBIN\", \"hosts\": [ { \"socket_address\": {\"address\": \"istio-pilot.istio-system\", \"port_value\": 15010} } ], \"circuit_breakers\": { \"thresholds\": [ { \"priority\": \"default\", \"max_connections\": \"100000\", \"max_pending_requests\": \"100000\", \"max_requests\": \"100000\" }, { \"priority\": \"high\", \"max_connections\": \"100000\", \"max_pending_requests\": \"100000\", \"max_requests\": \"100000\" }] }, \"upstream_connection_options\": { \"tcp_keepalive\": { \"keepalive_time\": 300 } }, \"http2_protocol_options\": { } } , { \"name\": \"zipkin\", \"type\": \"STRICT_DNS\", \"connect_timeout\": { \"seconds\": 1 }, \"lb_policy\": \"ROUND_ROBIN\", \"hosts\": [ { \"socket_address\": {\"address\": \"zipkin.istio-system\", \"port_value\": 9411} } ] } ] } ##### Tracing 配置分布式链路跟踪。 \"tracing\": { \"http\": { \"name\": \"envoy.zipkin\", \"config\": { \"collector_cluster\": \"zipkin\" } } } ##### Stats_sinks 这里配置的是和Envoy直连的metrics收集sink,和Mixer telemetry没有关系。Envoy自带stats格式的metrics上报。 \"stats_sinks\": [ { \"name\": \"envoy.statsd\", \"config\": { \"address\": { \"socket_address\": {\"address\": \"10.103.219.158\", \"port_value\": 9125} } } } ] 在Gist https://gist.github.com/zhaohuabing/14191bdcf72e37bf700129561c3b41ae中可以查看该配置文件的完整内容。 ## Envoy配置分析 ### 通过管理接口获取完整配置 从Envoy初始化配置文件中，我们可以大致看到Istio通过Envoy来实现服务发现和流量管理的基本原理。即控制面将xDS server信息通过static resource的方式配置到Envoy的初始化配置文件中，Envoy启动后通过xDS server获取到dynamic resource，包括网格中的service信息及路由规则。 Envoy配置初始化流程： ![](https://zhaohuabing.com/img/2018-09-25-istio-traffic-management-impl-intro/envoy-config-init.png) 1. Pilot-agent根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件envoy-rev0.json，该文件告诉Envoy从xDS server中获取动态配置信息，并配置了xDS server的地址信息，即控制面的Pilot。 1. Pilot-agent使用envoy-rev0.json启动Envoy进程。 1. Envoy根据初始配置获得Pilot地址，采用xDS接口从Pilot获取到Listener，Cluster，Route等d动态配置信息。 1. Envoy根据获取到的动态配置启动Listener，并根据Listener的配置，结合Route和Cluster对拦截到的流量进行处理。 可以看到，Envoy中实际生效的配置是由初始化配置文件中的静态配置和从Pilot获取的动态配置一起组成的。因此只对envoy-rev0 .json进行分析并不能看到Mesh中流量管理的全貌。那么有没有办法可以看到Envoy中实际生效的完整配置呢？答案是可以的，我们可以通过Envoy的管理接口来获取Envoy的完整配置。 kubectl exec -it productpage-v1-54b8b9f55-bx2dq -c istio-proxy curl http://127.0.0.1:15000/config_dump > config_dump 该文件内容长达近7000行，本文中就不贴出来了，在Gist https://gist.github.com/zhaohuabing/034ef87786d290a4e89cd6f5ad6fcc97 中可以查看到全文。 ### Envoy配置文件结构 ![](https://zhaohuabing.com/img/2018-09-25-istio-traffic-management-impl-intro/envoy-config.png) 文件中的配置节点包括： #### Bootstrap 从名字可以大致猜出这是Envoy的初始化配置，打开该节点，可以看到文件中的内容和前一章节中介绍的envoy-rev0.json是一致的，这里不再赘述。 ![](https://zhaohuabing.com/img/2018-09-25-istio-traffic-management-impl-intro/envoy-config-bootstrap.png) #### Clusters 在Envoy中，Cluster是一个服务集群，Cluster中包含一个到多个endpoint，每个endpoint都可以提供服务，Envoy根据负载均衡算法将请求发送到这些endpoint中。 在Productpage的clusters配置中包含static_clusters和dynamic_active_clusters两部分，其中static_clusters是来自于envoy-rev0.json的xDS server和zipkin server信息。dynamic_active_clusters是通过xDS接口从Istio控制面获取的动态服务信息。 ![](https://zhaohuabing.com/img/2018-09-25-istio-traffic-management-impl-intro/envoy-config-clusters.png) Dynamic Cluster中有以下几类Cluster： ##### Outbound Cluster 这部分的Cluster占了绝大多数，该类Cluster对应于Envoy所在节点的外部服务。以details为例，对于Productpage来说,details是一个外部服务，因此其Cluster名称中包含outbound字样。 从details 服务对应的cluster配置中可以看到，其类型为EDS，即表示该Cluster的endpoint来自于动态发现，动态发现中eds_config则指向了ads，最终指向static Resource中配置的xds-grpc cluster,即Pilot的地址。 { \"version_info\": \"2018-09-06T09:34:19Z\", \"cluster\": { \"name\": \"outbound|9080||details.default.svc.cluster.local\", \"type\": \"EDS\", \"eds_cluster_config\": { \"eds_config\": { \"ads\": {} }, \"service_name\": \"outbound|9080||details.default.svc.cluster.local\" }, \"connect_timeout\": \"1s\", \"circuit_breakers\": { \"thresholds\": [ {} ] } }, \"last_updated\": \"2018-09-06T09:34:20.404Z\" } 可以通过Pilot的调试接口获取该Cluster的endpoint： curl http://10.96.8.103:9093/debug/edsz > pilot_eds_dump 导出的文件长达1300多行，本文只贴出details服务相关的endpoint配置，完整文件参见:https://gist.github.com/zhaohuabing/a161d2f64746acd18097b74e6a5af551 从下面的文件内容可以看到，details cluster配置了1个endpoint地址，是details的pod ip。 { \"clusterName\": \"outbound|9080||details.default.svc.cluster.local\", \"endpoints\": [ { \"locality\": { }, \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"192.168.206.21\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"uid\": \"kubernetes://details-v1-6764bbc7f7-qwzdg.default\" } } } } ] } ] } ##### Inbound Cluster 该类Cluster对应于Envoy所在节点上的服务。如果该服务接收到请求，当然就是一个入站请求。对于Productpage Pod上的Envoy，其对应的Inbound Cluster只有一个，即productpage。该cluster对应的host为127.0.0.1,即环回地址上productpage的监听端口。由于iptable规则中排除了127.0.0.1,入站请求通过该Inbound cluster处理后将跳过Envoy，直接发送给Productpage进程处理。 { \"version_info\": \"2018-09-14T01:44:05Z\", \"cluster\": { \"name\": \"inbound|9080||productpage.default.svc.cluster.local\", \"connect_timeout\": \"1s\", \"hosts\": [ { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 9080 } } ], \"circuit_breakers\": { \"thresholds\": [ {} ] } }, \"last_updated\": \"2018-09-14T01:44:05.291Z\" } ##### BlackHoleCluster 这是一个特殊的Cluster，并没有配置后端处理请求的Host。如其名字所暗示的一样，请求进入后将被直接丢弃掉。如果一个请求没有找到其对的目的服务，则被发到cluste。 { \"version_info\": \"2018-09-06T09:34:19Z\", \"cluster\": { \"name\": \"BlackHoleCluster\", \"connect_timeout\": \"5s\" }, \"last_updated\": \"2018-09-06T09:34:20.408Z\" } #### Listeners Envoy采用listener来接收并处理downstream发过来的请求，listener的处理逻辑是插件式的，可以通过配置不同的filter来插入不同的处理逻辑。Istio就在Envoy中加入了用于policy check和metric report的Mixer filter。 Listener可以绑定到IP Socket或者Unix Domain Socket上，也可以不绑定到一个具体的端口上，而是接收从其他listener转发来的数据。Istio就是利用了Envoy listener的这一特点实现了将来发向不同服务的请求转交给不同的listener处理。 ##### Virtual Listener Envoy创建了一个在15001端口监听的入口监听器。Iptable将请求截取后发向15001端口，该监听器接收后并不进行业务处理，而是根据请求目的地分发给其他监听器处理。该监听器取名为\"virtual\"（虚拟）监听器也是这个原因。 Envoy是如何做到按服务分发的呢？ 可以看到该Listener的配置项use_original_dest设置为true,该配置要求监听器将接收到的请求转交给和请求原目的地址关联的listener进行处理。 从其filter配置可以看到，如果找不到和请求目的地配置的listener进行转交，则请求将被发送到[BlackHoleCluster](#blackholecluster),由于BlackHoleCluster并没有配置host，因此找不到对应目的地对应监听器的请求实际上会被丢弃。 { \"version_info\": \"2018-09-06T09:34:19Z\", \"listener\": { \"name\": \"virtual\", \"address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 15001 } }, \"filter_chains\": [ { \"filters\": [ { \"name\": \"envoy.tcp_proxy\", \"config\": { \"stat_prefix\": \"BlackHoleCluster\", \"cluster\": \"BlackHoleCluster\" } } ] } ], \"use_original_dst\": true }, \"last_updated\": \"2018-09-06T09:34:26.262Z\" } ##### Inbound Listener 在Productpage Pod上的Envoy创建了Listener 192.168.206.23_9080，当外部调用Productpage服务的请求到达Pod上15001的\"Virtual\" Listener时，Virtual Listener根据请求目的地匹配到该Listener,请求将被转发过来。 { \"version_info\": \"2018-09-14T01:44:05Z\", \"listener\": { \"name\": \"192.168.206.23_9080\", \"address\": { \"socket_address\": { \"address\": \"192.168.206.23\", \"port_value\": 9080 } }, \"filter_chains\": [ { \"filters\": [ { \"name\": \"mixer\", \"config\": { \"transport\": { \"check_cluster\": \"outbound|9091||istio-policy.istio-system.svc.cluster.local\", \"network_fail_policy\": { \"policy\": \"FAIL_CLOSE\" }, \"report_cluster\": \"outbound|9091||istio-telemetry.istio-system.svc.cluster.local\", \"attributes_for_mixer_proxy\": { \"attributes\": { \"source.uid\": { \"string_value\": \"kubernetes://productpage-v1-54b8b9f55-bx2dq.default\" } } } }, \"mixer_attributes\": { \"attributes\": { \"destination.port\": { \"int64_value\": \"9080\" }, \"context.reporter.uid\": { \"string_value\": \"kubernetes://productpage-v1-54b8b9f55-bx2dq.default\" }, \"destination.namespace\": { \"string_value\": \"default\" }, \"destination.ip\": { \"bytes_value\": \"AAAAAAAAAAAAAP//wKjOFw==\" }, \"destination.uid\": { \"string_value\": \"kubernetes://productpage-v1-54b8b9f55-bx2dq.default\" }, \"context.reporter.kind\": { \"string_value\": \"inbound\" } } } } }, { \"name\": \"envoy.tcp_proxy\", \"config\": { \"stat_prefix\": \"inbound|9080||productpage.default.svc.cluster.local\", \"cluster\": \"inbound|9080||productpage.default.svc.cluster.local\" } } ] } ], \"deprecated_v1\": { \"bind_to_port\": false } }, \"last_updated\": \"2018-09-14T01:44:05.754Z\" } 从上面的配置\"bind_to_port\": false可以得知该listener创建后并不会被绑定到tcp端口上直接接收网络上的数据，因此其所有请求都转发自15001端口。 该listener配置的envoy.tcp_proxy filter对应的cluster为[\"inbound|9080||productpage.default.svc.cluster.local\"](#inbound-cluster),该cluster配置的host为127.0.0.1:9080，因此Envoy会将该请求发向127.0.0.1:9080。由于iptable设置中127.0.0.1不会被拦截,该请求将发送到Productpage进程的9080端口进行业务处理。 除此以外，Listenter中还包含Mixer filter的配置信息，配置了策略检查(Mixer check)和Metrics上报(Mixer report)服务器地址，以及Mixer上报的一些attribute取值。 ##### Outbound Listener Envoy为网格中的外部服务按端口创建多个Listener，以用于处理出向请求。 Productpage Pod中的Envoy创建了多个Outbound Listener * 0.0.0.0_9080 :处理对details,reviews和rating服务的出向请求 * 0.0.0.0_9411 :处理对zipkin的出向请求 * 0.0.0.0_15031 :处理对ingressgateway的出向请求 * 0.0.0.0_3000 :处理对grafana的出向请求 * 0.0.0.0_9093 :处理对citadel、galley、pilot、(Mixer)policy、(Mixer)telemetry的出向请求 * 0.0.0.0_15004 :处理对(Mixer)policy、(Mixer)telemetry的出向请求 * ...... 除了9080这个Listener用于处理应用的业务之外，其他listener都是Istio用于处理自身组件之间通信使用的，有的控制面组件如Pilot，Mixer对应多个listener，是因为该组件有多个端口提供服务。 我们这里主要分析一下9080这个业务端口的Listenrer。和Outbound Listener一样，该Listener同样配置了\"bind_to_port\": false属性，因此该listener也没有被绑定到tcp端口上，其接收到的所有请求都转发自15001端口的Virtual listener。 监听器name为0.0.0.0_9080,推测其含义应为匹配发向任意IP的9080的请求，从[bookinfo程序结构](#bookinfo程序结构)可以看到该程序中的productpage,revirews,ratings,details四个service都是9080端口，那么Envoy如何区别处理这四个service呢？ 首先需要区分入向（发送给productpage）请求和出向（发送给其他几个服务）请求： * 发给productpage的入向请求，virtual listener根据其目的IP和Port首先匹配到[192.168.206.23_9080](#inbound-listener)这个listener上，不会进入0.0.0.0_9080 listener处理。 * 从productpage外发给reviews、details和ratings的出向请求，virtual listener无法找到和其目的IP完全匹配的listener，因此根据通配原则转交给0.0.0.0_9080处理。 > 备注： 1. 该转发逻辑为根据Envoy配置进行的推测，并未分析Envoy代码进行验证。欢迎了解Envoy代码和实现机制的朋友指正。 2.根据业务逻辑，实际上productpage并不会调用ratings服务，但Istio并不知道各个业务之间会如何调用，因此将所有的服务信息都下发到了Envoy中。这样做对效率和性能理论上有一定影响，存在一定的优化空间。 由于对应到reviews、details和Ratings三个服务，当0.0.0.0_9080接收到出向请求后，并不能直接发送到一个downstream cluster中，而是需要根据请求目的地进行不同的路由。 在该listener的配置中，我们可以看到并没有像inbound listener那样通过envoy.tcp_proxy直接指定一个downstream的cluster，而是通过rds配置了一个[路由规则9080](#routes)，在路由规则中再根据不同的请求目的地对请求进行处理。 { \"version_info\": \"2018-09-06T09:34:19Z\", \"listener\": { \"name\": \"0.0.0.0_9080\", \"address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 9080 } }, \"filter_chains\": [ { \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"config\": { \"access_log\": [ { \"name\": \"envoy.file_access_log\", \"config\": { \"path\": \"/dev/stdout\" } } ], \"http_filters\": [ { \"name\": \"mixer\", \"config\": { ...... } }, { \"name\": \"envoy.cors\" }, { \"name\": \"envoy.fault\" }, { \"name\": \"envoy.router\" } ], \"tracing\": { \"operation_name\": \"EGRESS\", \"client_sampling\": { \"value\": 100 }, \"overall_sampling\": { \"value\": 100 }, \"random_sampling\": { \"value\": 100 } }, \"use_remote_address\": false, \"stat_prefix\": \"0.0.0.0_9080\", \"rds\": { \"route_config_name\": \"9080\", \"config_source\": { \"ads\": {} } }, \"stream_idle_timeout\": \"0.000s\", \"generate_request_id\": true, \"upgrade_configs\": [ { \"upgrade_type\": \"websocket\" } ] } } ] } ], \"deprecated_v1\": { \"bind_to_port\": false } }, \"last_updated\": \"2018-09-06T09:34:26.172Z\" }, #### Routes 配置Envoy的路由规则。Istio下发的缺省路由规则中对每个端口设置了一个路由规则，根据host来对请求进行路由分发。 下面是9080的路由配置，从文件中可以看到对应了3个virtual host，分别是details、ratings和reviews，这三个virtual host分别对应到不同的[outbound cluster](#outbound-cluster)。 { \"version_info\": \"2018-09-14T01:38:20Z\", \"route_config\": { \"name\": \"9080\", \"virtual_hosts\": [ { \"name\": \"details.default.svc.cluster.local:9080\", \"domains\": [ \"details.default.svc.cluster.local\", \"details.default.svc.cluster.local:9080\", \"details\", \"details:9080\", \"details.default.svc.cluster\", \"details.default.svc.cluster:9080\", \"details.default.svc\", \"details.default.svc:9080\", \"details.default\", \"details.default:9080\", \"10.101.163.201\", \"10.101.163.201:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||details.default.svc.cluster.local\", \"timeout\": \"0s\", \"max_grpc_timeout\": \"0s\" }, \"decorator\": { \"operation\": \"details.default.svc.cluster.local:9080/*\" }, \"per_filter_config\": { \"mixer\": { ...... } } } ] }, { \"name\": \"ratings.default.svc.cluster.local:9080\", \"domains\": [ \"ratings.default.svc.cluster.local\", \"ratings.default.svc.cluster.local:9080\", \"ratings\", \"ratings:9080\", \"ratings.default.svc.cluster\", \"ratings.default.svc.cluster:9080\", \"ratings.default.svc\", \"ratings.default.svc:9080\", \"ratings.default\", \"ratings.default:9080\", \"10.99.16.205\", \"10.99.16.205:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||ratings.default.svc.cluster.local\", \"timeout\": \"0s\", \"max_grpc_timeout\": \"0s\" }, \"decorator\": { \"operation\": \"ratings.default.svc.cluster.local:9080/*\" }, \"per_filter_config\": { \"mixer\": { ...... }, \"disable_check_calls\": true } } } ] }, { \"name\": \"reviews.default.svc.cluster.local:9080\", \"domains\": [ \"reviews.default.svc.cluster.local\", \"reviews.default.svc.cluster.local:9080\", \"reviews\", \"reviews:9080\", \"reviews.default.svc.cluster\", \"reviews.default.svc.cluster:9080\", \"reviews.default.svc\", \"reviews.default.svc:9080\", \"reviews.default\", \"reviews.default:9080\", \"10.108.25.157\", \"10.108.25.157:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||reviews.default.svc.cluster.local\", \"timeout\": \"0s\", \"max_grpc_timeout\": \"0s\" }, \"decorator\": { \"operation\": \"reviews.default.svc.cluster.local:9080/*\" }, \"per_filter_config\": { \"mixer\": { ...... }, \"disable_check_calls\": true } } } ] } ], \"validate_clusters\": false }, \"last_updated\": \"2018-09-27T07:17:50.242Z\" } ## Bookinfo端到端调用分析 通过前面章节对Envoy配置文件的分析，我们了解到Istio控制面如何将服务和路由信息通过xDS接口下发到数据面中；并介绍了Envoy上生成的各种配置数据的结构，包括listener,cluster,route和endpoint。 下面我们来分析一个端到端的调用请求，通过调用请求的流程把这些配置串连起来，以从全局上理解Istio控制面的流量控制是如何在数据面的Envoy上实现的。 下图描述了一个Productpage服务调用Details服务的请求流程： ![](https://zhaohuabing.com/img/2018-09-25-istio-traffic-management-impl-intro/envoy-traffic-route.png) 1. Productpage发起对Details的调用：`http://details:9080/details/0` 。 2. 请求被Pod的iptable规则拦截，转发到15001端口。 3. Envoy的Virtual Listener在15001端口上监听，收到了该请求。 4. 请求被Virtual Listener根据原目标IP（通配）和端口（9080）转发到0.0.0.0_9080这个listener。 { \"version_info\": \"2018-09-06T09:34:19Z\", \"listener\": { \"name\": \"virtual\", \"address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 15001 } } ...... \"use_original_dst\": true //请求转发给和原始目的IP:Port匹配的listener }, 5. 根据0.0.0.0_9080 listener的http_connection_manager filter配置,该请求采用“9080” route进行分发。 { \"version_info\": \"2018-09-06T09:34:19Z\", \"listener\": { \"name\": \"0.0.0.0_9080\", \"address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 9080 } }, \"filter_chains\": [ { \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"config\": { ...... \"rds\": { \"route_config_name\": \"9080\", \"config_source\": { \"ads\": {} } }, } ] } ], \"deprecated_v1\": { \"bind_to_port\": false } }, \"last_updated\": \"2018-09-06T09:34:26.172Z\" }, { }, 6. “9080”这个route的配置中，host name为details:9080的请求对应的cluster为outbound|9080||details.default.svc.cluster.local { \"version_info\": \"2018-09-14T01:38:20Z\", \"route_config\": { \"name\": \"9080\", \"virtual_hosts\": [ { \"name\": \"details.default.svc.cluster.local:9080\", \"domains\": [ \"details.default.svc.cluster.local\", \"details.default.svc.cluster.local:9080\", \"details\", \"details:9080\", \"details.default.svc.cluster\", \"details.default.svc.cluster:9080\", \"details.default.svc\", \"details.default.svc:9080\", \"details.default\", \"details.default:9080\", \"10.101.163.201\", \"10.101.163.201:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||details.default.svc.cluster.local\", \"timeout\": \"0s\", \"max_grpc_timeout\": \"0s\" }, ...... } } } ] }, ...... { }, 7. outbound|9080||details.default.svc.cluster.local cluster为动态资源，通过eds查询得到其endpoint为192.168.206.21:9080。 { \"clusterName\": \"outbound|9080||details.default.svc.cluster.local\", \"endpoints\": [ { \"locality\": { }, \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"192.168.206.21\", \"portValue\": 9080 } } }, ...... } ] } ] } 8. 请求被转发到192.168.206.21，即Details服务所在的Pod，被iptable规则拦截，转发到15001端口。 9. Envoy的Virtual Listener在15001端口上监听，收到了该请求。 10. 请求被Virtual Listener根据请求原目标地址IP（192.168.206.21）和端口（9080）转发到192.168.206.21_9080这个listener。 11. 根据92.168.206.21_9080 listener的http_connection_manager filter配置,该请求对应的cluster为 inbound|9080||details.default.svc.cluster.local 。 { \"version_info\": \"2018-09-06T09:34:16Z\", \"listener\": { \"name\": \"192.168.206.21_9080\", \"address\": { \"socket_address\": { \"address\": \"192.168.206.21\", \"port_value\": 9080 } }, \"filter_chains\": [ { \"filters\": [ { \"name\": \"envoy.http_connection_manager\", ...... \"route_config\": { \"name\": \"inbound|9080||details.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"name\": \"inbound|http|9080\", \"routes\": [ ...... \"route\": { \"max_grpc_timeout\": \"0.000s\", \"cluster\": \"inbound|9080||details.default.svc.cluster.local\", \"timeout\": \"0.000s\" }, ...... \"match\": { \"prefix\": \"/\" } } ], \"domains\": [ \"*\" ] } ] }, ...... ] } } ] } ], \"deprecated_v1\": { \"bind_to_port\": false } }, \"last_updated\": \"2018-09-06T09:34:22.184Z\" } ``` inbound|9080||details.default.svc.cluster.local cluster配置的host为127.0.0.1：9080。 请求被转发到127.0.0.1：9080，即Details服务进行处理。 上述调用流程涉及的完整Envoy配置文件参见： Proudctpage：https://gist.github.com/zhaohuabing/034ef87786d290a4e89cd6f5ad6fcc97 Details：https://gist.github.com/zhaohuabing/544d4d45447b65d10150e528a190f8ee 小结 本文介绍了Istio流量管理相关组件，Istio控制面和数据面之间的标准接口，以及Istio下发到Envoy的完整配置数据的结构和内容。然后通过Bookinfo示例程序的一个端到端调用分析了Envoy是如何实现服务网格中服务发现和路由转发的，希望能帮助大家透过概念更进一步深入理解Istio流量管理的实现机制。 参考资料 Istio Traffic Managment Concept Data Plane API kubernetes Custom Resource Istio Pilot Design Overview Envoy V2 API Overview Data Plane API Protocol Buffer Definition xDS REST and gRPC protocol https://github.com/istio/istio/tree/master/pilot/pkg/proxy/envoy/v2 Pilot Debug interface Istio Sidecar自动注入原理 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 22:42:56 "},"content/ingress/":{"url":"content/ingress/","title":"如何为服务网格选择入口网关","keywords":"","body":"在启用了Istio服务网格的Kubernetes集群中，缺省情况下只能在集群内部访问网格中的服务，要如何才能从外部网络访问这些服务呢？ Kubernetes和Istio提供了NodePort，LoadBalancer，Kubernetes Ingress，Istio Gateway等多种外部流量入口的方式，面对这么多种方式，我们在产品部署中应该如何选择？ 本章节将对Kubernetes和Istio对外提供服务的各种方式进行详细介绍和对比分析，并根据分析结果提出一个可用于产品部署的解决方案。 说明：阅读本章节要求读者了解Kubernetes和Istio的基本概念，包括Pod、Service、NodePort、LoadBalancer、Ingress、Gateway、VirtualService等。如对这些概念不熟悉，可以在阅读过程中参考文后的相关链接。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/ingress/cluster-ip.html":{"url":"content/ingress/cluster-ip.html","title":"内部通讯-ClusterIP","keywords":"","body":"内部服务间的通信 首先，我们来回顾一下Kubernetes集群内部各个服务之间相互访问的方法。 Cluster IP Kubernetes以Pod作为应用部署的最小单位。kubernetes会根据Pod的声明对其进行调度，包括创建、销毁、迁移、水平伸缩等，因此Pod 的IP地址不是固定的，不方便直接采用Pod IP对服务进行访问。 为解决该问题，Kubernetes提供了Service资源，Service对提供同一个服务的多个Pod进行聚合。一个Service提供一个虚拟的Cluster IP，后端对应一个或者多个提供服务的Pod。在集群中访问该Service时，采用Cluster IP即可，Kube-proxy负责将发送到Cluster IP的请求转发到后端的Pod上。 Kube-proxy是一个运行在每个节点上的go应用程序，支持三种工作模式： userspace 该模式下kube-proxy会为每一个Service创建一个监听端口。发向Cluster IP的请求被Iptables规则重定向到Kube-proxy监听的端口上，Kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。 该模式下，Kube-proxy充当了一个四层Load balancer的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加两次内核和用户空间之间的数据拷贝，效率较另外两种模式低一些；好处是当后端的Pod不可用时，kube-proxy可以重试其他Pod。 图片 - Kube-proxy userspace模式 图片来自：Kubernetes官网文档 iptables 为了避免增加内核和用户空间的数据拷贝操作，提高转发效率，Kube-proxy提供了iptables模式。在该模式下，Kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。 该模式下Kube-proxy不承担四层代理的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。 图片 - Kube-proxy iptables模式 图片来自：Kubernetes官网文档 ipvs 该模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs rules。ipvs也是在kernel模式下通过netfilter实现的，但采用了hash table来存储规则，因此在规则较多的情况下，Ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。如果要设置kube-proxy为ipvs模式，必须在操作系统中安装IPVS内核模块。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/ingress/istio-sidecar-proxy.html":{"url":"content/ingress/istio-sidecar-proxy.html","title":"内部通讯-Sidecar Proxy","keywords":"","body":"Istio Sidecar Proxy Cluster IP解决了服务之间相互访问的问题，但从上面Kube-proxy的三种模式可以看到，Cluster IP的方式只提供了服务发现和基本的LB功能。如果要为服务间的通信应用灵活的路由规则以及提供Metrics collection，distributed tracing等服务管控功能,就必须得依靠Istio提供的服务网格能力了。 在Kubernetes中部署Istio后，Istio通过iptables和Sidecar Proxy接管服务之间的通信，服务间的相互通信不再通过Kube-proxy，而是通过Istio的Sidecar Proxy进行。请求流程是这样的：Client发起的请求被iptables重定向到Sidecar Proxy，Sidecar Proxy根据从控制面获取的服务发现信息和路由规则，选择一个后端的Server Pod创建链接，代理并转发Client的请求。 Istio Sidecar Proxy和Kube-proxy的userspace模式的工作机制类似，都是通过在用户空间的一个代理来实现客户端请求的转发和后端多个Pod之间的负载均衡。两者的不同点是：Kube-Proxy工作在四层，而Sidecar Proxy则是一个七层代理，可以针对HTTP，GRPS等应用层的语义进行处理和转发，因此功能更为强大，可以配合控制面实现更为灵活的路由规则和服务管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/ingress/nodeport.html":{"url":"content/ingress/nodeport.html","title":"外部通信-NodePort","keywords":"","body":"如何从外部网络访问 Kubernetes的Pod IP和Cluster IP都只能在集群内部访问，而我们通常需要从外部网络上访问集群中的某些服务，Kubernetes提供了下述几种方式来为集群提供外部流量入口。 NodePort NodePort在集群中的主机节点上为Service提供一个代理端口，以允许从主机网络上对Service进行访问。Kubernetes官网文档只介绍了NodePort的功能，并未对其实现原理进行解释。下面我们通过实验来分析NodePort的实现机制。 www.katacoda.com 这个网站提供了一个交互式的Kubernetes playground，注册即可免费实验kubernetes的相关功能，下面我们就使用Katacoda来分析Nodeport的实现原理。 在浏览器中输入这个网址：https://www.katacoda.com/courses/kubernetes/networking-introduction， 打开后会提供了一个实验用的Kubernetes集群，并可以通过网元模拟Terminal连接到集群的Master节点。 执行下面的命令创建一个nodeport类型的service。 kubectl apply -f nodeport.yaml 查看创建的service，可以看到kubernetes创建了一个名为webapp-nodeport-svc的service，并为该service在主机节点上创建了30080这个Nodeport。 master $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 36m webapp1-nodeport-svc NodePort 10.103.188.73 80:30080/TCP 3m webapp-nodeport-svc后端对应两个Pod，其Pod的IP分别为10.32.0.3和10.32.0.5。 master $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IPNODE NOMINATED NODE webapp1-nodeport-deployment-785989576b-cjc5b 1/1 Running 0 2m 10.32.0.3 webapp1-nodeport-deployment-785989576b-tpfqr 1/1 Running 0 2m 10.32.0.5 通过netstat命令可以看到Kube-proxy在主机网络上创建了30080监听端口，用于接收从主机网络进入的外部流量。 master $ netstat -lnp|grep 30080 tcp6 0 0 :::30080 :::* LISTEN 7427/kube-proxy 下面是Kube-proxy创建的相关iptables规则以及对应的说明。可以看到Kube-proxy为Nodeport创建了相应的IPtable规则，将发向30080这个主机端口上的流量重定向到了后端的两个Pod IP上。 iptables-save > iptables-dump # Generated by iptables-save v1.6.0 on Thu Mar 28 07:33:57 2019 *nat # Nodeport规则链 :KUBE-NODEPORTS - [0:0] # Service规则链 :KUBE-SERVICES - [0:0] # Nodeport和Service共用的规则链 :KUBE-SVC-J2DWGRZTH4C2LPA4 - [0:0] :KUBE-SEP-4CGFRVESQ3AECDE7 - [0:0] :KUBE-SEP-YLXG4RMKAICGY2B3 - [0:0] # 将host上30080端口的外部tcp流量转到KUBE-SVC-J2DWGRZTH4C2LPA4链 -A KUBE-NODEPORTS -p tcp -m comment --comment \"default/webapp1-nodeport-svc:\" -m tcp --dport 30080 -j KUBE-SVC-J2DWGRZTH4C2LPA4 #将发送到Cluster IP 10.103.188.73的内部流量转到KUBE-SVC-J2DWGRZTH4C2LPA4链 KUBE-SERVICES -d 10.103.188.73/32 -p tcp -m comment --comment \"default/webapp1-nodeport-svc: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-J2DWGRZTH4C2LPA4 #将发送到webapp1-nodeport-svc的流量转交到第一个Pod（10.32.0.3）相关的规则链上，比例为50% -A KUBE-SVC-J2DWGRZTH4C2LPA4 -m comment --comment \"default/webapp1-nodeport-svc:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-YLXG4RMKAICGY2B3 #将发送到webapp1-nodeport-svc的流量转交到第二个Pod（10.32.0.5）相关的规则链上 -A KUBE-SVC-J2DWGRZTH4C2LPA4 -m comment --comment \"default/webapp1-nodeport-svc:\" -j KUBE-SEP-4CGFRVESQ3AECDE7 #将请求重定向到Pod 10.32.0.3 -A KUBE-SEP-YLXG4RMKAICGY2B3 -p tcp -m comment --comment \"default/webapp1-nodeport-svc:\" -m tcp -j DNAT --to-destination 10.32.0.3:80 #将请求重定向到Pod 10.32.0.5 -A KUBE-SEP-4CGFRVESQ3AECDE7 -p tcp -m comment --comment \"default/webapp1-nodeport-svc:\" -m tcp -j DNAT --to-destination 10.32.0.5:80 从上面的实验可以看到，通过将一个Service定义为NodePort类型，Kubernetes会通过集群中node上的Kube-proxy为该Service在主机网络上创建一个监听端口。Kube-proxy并不会直接接收该主机端口进入的流量，而是会创建相应的Iptables规则，并通过Iptables将从该端口收到的流量直接转发到后端的Pod中。 NodePort的流量转发机制和Cluster IP的iptables模式类似，唯一不同之处是在主机网络上开了一个“NodePort”来接受外部流量。从上面的规则也可以看出，在创建Nodeport时，Kube-proxy也会同时为Service创建Cluster IP相关的iptables规则。 备注：除采用iptables进行流量转发，NodePort应该也可以提供userspace模式以及ipvs模式，这里未就这两种模式进行实验验证。 从分析得知，在NodePort模式下，集群内外部的通讯如下图所示： Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/ingress/loadbalancer.html":{"url":"content/ingress/loadbalancer.html","title":"外部通讯-LoadBalancer","keywords":"","body":"LoadBalancer NodePort提供了一种从外部网络访问Kubernetes集群内部Service的方法，但该方法存在下面一些限制，导致这种方式主要适用于程序开发，不适合用于产品部署。 Kubernetes cluster host的IP必须是一个well-known IP，即客户端必须知道该IP。但Cluster中的host是被作为资源池看待的，可以增加删除，每个host的IP一般也是动态分配的，因此并不能认为host IP对客户端而言是well-known IP。 客户端访问某一个固定的host IP的方式存在单点故障。假如一台host宕机了，kubernetes cluster会把应用 reload到另一节点上，但客户端就无法通过该host的nodeport访问应用了。 通过一个主机节点作为网络入口，在网络流量较大时存在性能瓶颈。 为了解决这些问题，Kubernetes提供了LoadBalancer。通过将Service定义为LoadBalancer类型，Kubernetes在主机节点的NodePort前提供了一个四层的负载均衡器。该四层负载均衡器负责将外部网络流量分发到后面的多个节点的NodePort端口上。 下图展示了Kubernetes如何通过LoadBalancer方式对外提供流量入口，图中LoadBalancer后面接入了两个主机节点上的NodePort，后端部署了三个Pod提供服务。根据集群的规模，可以在LoadBalancer后面可以接入更多的主机节点，以进行负荷分担。 图片 - NodeBalancer 备注：LoadBalancer类型需要云服务提供商的支持，Service中的定义只是在Kubernetes配置文件中提出了一个要求，即为该Service创建Load Balancer，至于如何创建则是由Google Cloud或Amazon Cloud等云服务商提供的，创建的Load Balancer的过程不在Kubernetes Cluster的管理范围中。 目前WS, Azure, CloudStack, GCE 和 OpenStack 等主流的公有云和私有云提供商都可以为Kubernetes提供Load Balancer。一般来说，公有云提供商还会为Load Balancer提供一个External IP，以提供Internet接入。如果你的产品没有使用云提供商，而是自建Kubernetes Cluster，则需要自己提供LoadBalancer。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/ingress/ingress.html":{"url":"content/ingress/ingress.html","title":"外部通讯-Ingress","keywords":"","body":"Ingress LoadBalancer类型的Service提供的是四层负载均衡器，当只需要向外暴露一个服务的时候，采用这种方式是没有问题的。但当一个应用需要对外提供多个服务时，采用该方式则要求为每一个四层服务（IP+Port）都创建一个外部load balancer。 一般来说，同一个应用的多个服务/资源会放在同一个域名下，在这种情况下，创建多个Load balancer是完全没有必要的，反而带来了额外的开销和管理成本。另外直接将服务暴露给外部用户也会导致了前端和后端的耦合，影响了后端架构的灵活性，如果以后由于业务需求对服务进行调整会直接影响到客户端。为了解决该问题，可以通过使用Kubernetes Ingress来作为网络入口。 Ingress 功能介绍 Kubernetes Ingress声明了一个应用层（OSI七层）的负载均衡器，可以根据HTTP请求的内容将来自同一个TCP端口的请求分发到不同的Kubernetes Service，其功能包括： 按HTTP请求的URL进行路由 同一个TCP端口进来的流量可以根据URL路由到Cluster中的不同服务，如下图所示： 图片 - Simple fanout 按HTTP请求的Host进行路由 同一个IP进来的流量可以根据HTTP请求的Host路由到Cluster中的不同服务，如下图所示： 图片 - Name based virtual hosting Ingress 规则定义了对七层网关的要求，包括URL分发规则，基于不同域名的虚拟主机，SSL证书等。Kubernetes使用Ingress Controller 来监控Ingress规则，并通过一个七层网关来实现这些要求，一般可以使用Nginx，HAProxy，Envoy等。 Ingress配合NodePort和LoadBalancer提供对外流量入口 虽然Ingress Controller通过七层网关为后端的多个Service提供了统一的入口，但由于其部署在集群中，因此并不能直接对外提供服务。实际上Ingress需要配合NodePort和LoadBalancer才能提供对外的流量入口，如下图所示： 图片 - 采用Ingress, NodePortal和LoadBalancer提供外部流量入口的拓扑结构 上图描述了如何采用Ingress配合NodePort和Load Balancer为集群提供外部流量入口，从该拓扑图中可以看到该架构的伸缩性非常好，在NodePort，Ingress，Pod等不同的接入层面都可以对系统进行水平扩展，以应对不同的外部流量要求。 上图只展示了逻辑架构，下面的图展示了具体的实现原理： 图片 - 采用Ingress, NodePortal和LoadBalancer提供外部流量入口的实现原理 流量从外部网络到达Pod的完整路径如下： 外部请求先通过四层Load Balancer进入内部网络 Load Balancer将流量分发到后端多个主机节点上的NodePort (userspace转发) 请求从NodePort进入到Ingress Controller (iptabes规则，Ingress Controller本身是一个NodePort类型的Service) Ingress Controller根据Ingress rule进行七层分发，根据HTTP的URL和Host将请求分发给不同的Service (userspace转发) Service将请求最终导入到后端提供服务的Pod中 (iptabes规则) 从前面的介绍可以看到，K8S Ingress提供了一个基础的七层网关功能的抽象定义，其作用是对外提供一个七层服务的统一入口，并根据URL/HOST将请求路由到集群内部不同的服务上。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/ingress/k8s-ingress-as-mesh-gateway.html":{"url":"content/ingress/k8s-ingress-as-mesh-gateway.html","title":"采用K8s Ingress作为网格的流量入口","keywords":"","body":"如何为服务网格选择入口网关？ 在Istio服务网格中，通过为每个Service部署一个sidecar代理，Istio接管了Service之间的请求流量。控制面可以对网格中的所有sidecar代理进行统一配置，实现了对网格内部流量的路由控制，从而可以实现灰度发布，流量镜像，故障注入等服务管控功能。但是，Istio并没有为入口网关提供一个较为完善的解决方案。 K8s Ingress 在0.8版本以前，Istio缺省采用K8s Ingress来作为Service Mesh的流量入口。K8s Ingress统一了应用的流量入口，但存在两个问题： K8s Ingress是独立在Istio体系之外的，需要单独采用Ingress rule进行配置，导致系统入口和内部存在两套互相独立的路由规则配置，运维和管理较为复杂。 K8s Ingress rule的功能较弱，不能在入口处实现和网格内部类似的路由规则，也不具备网格sidecar的其它能力，导致难以从整体上为应用系统实现灰度发布、分布式跟踪等服务管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/ingress/istio-gateway-as-mesh-gateway.html":{"url":"content/ingress/istio-gateway-as-mesh-gateway.html","title":"采用Istio Gateway作为网络的流量入口","keywords":"","body":"Istio Gateway Istio社区意识到了Ingress和Mesh内部配置割裂的问题，因此从0.8版本开始，社区采用了 Gateway 资源代替K8s Ingress来表示流量入口。 Istio Gateway资源本身只能配置L4-L6的功能，例如暴露的端口，TLS设置等；但Gateway可以和绑定一个VirtualService，在VirtualService 中可以配置七层路由规则，这些七层路由规则包括根据按照服务版本对请求进行导流，故障注入，HTTP重定向，HTTP重写等所有Mesh内部支持的路由规则。 Gateway和VirtualService用于表示Istio Ingress的配置模型，Istio Ingress的缺省实现则采用了和Sidecar相同的Envoy proxy。 通过该方式，Istio控制面用一致的配置模型同时控制了入口网关和内部的sidecar代理。这些配置包括路由规则，策略检查、Telementry收集以及其他服务管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/ingress/api-gateway-requirement.html":{"url":"content/ingress/api-gateway-requirement.html","title":"服务化应用对API Gateway的功能需求","keywords":"","body":"应用对API Gateway的需求 采用Gateway和VirtualService实现的Istio Ingress Gateway提供了网络入口处的基础通信功能，包括可靠的通信和灵活的路由规则。但对于一个服务化应用来说，网络入口除了基础的通讯功能之外，还有一些其他的应用层功能需求，例如： 第三方系统对API的访问控制 用户对系统的访问控制 修改请求/返回数据 服务API的生命周期管理 服务访问的SLA、限流及计费 …. 图片 - Kubernetes ingress, Istio gateway and API gateway的功能对比 API Gateway需求中很大一部分需要根据不同的应用系统进行定制，目前看来暂时不大可能被纳入K8s Ingress或者Istio Gateway的规范之中。为了满足这些需求，涌现出了各类不同的k8s Ingress Controller以及Istio Ingress Gateway实现，包括Ambassador ，Kong, Traefik,Solo等。 这些网关产品在实现在提供基础的K8s Ingress能力的同时，提供了强大的API Gateway功能，但由于缺少统一的标准，这些扩展实现之间相互之间并不兼容。而且遗憾的是，目前这些Ingress controller都还没有正式提供和Istio 控制面集成的能力。 备注： Ambassador将对Istio路由规则的支持纳入了Roadmap https://www.getambassador.io/user-guide/with-istio/ Istio声称支持Istio-Based Route Rule Discovery (尚处于实验阶段) https://gloo.solo.io/introduction/architecture/ Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/ingress/mesh-gateway-solution.html":{"url":"content/ingress/mesh-gateway-solution.html","title":"服务网格入口网关的解决方案","keywords":"","body":"采用API Gateway + Sidecar Proxy作为服务网格的流量入口 在目前难以找到一个同时具备API Gateway和Isito Ingress能力的网关的情况下，一个可行的方案是使用API Gateway和Sidecar Proxy一起为服务网格提供外部流量入口。 由于API Gateway已经具备七层网关的功能，Mesh Ingress中的Sidecar只需要提供VirtualService资源的路由能力，并不需要提供Gateway资源的网关能力，因此采用Sidecar Proxy即可。网络入口处的Sidecar Proxy和网格内部应用Pod中Sidecar Proxy的唯一一点区别是：该Sidecar只接管API Gateway向Mesh内部的流量，并不接管外部流向API Gateway的流量；而应用Pod中的Sidecar需要接管进入应用的所有流量。 图片 - 采用API Gateway + Sidecar Proxy为服务网格提供流量入口 备注：在实际部署时，API Gateway前端需要采用NodePort和LoadBalancer提供外部流量入口。为了突出主题，对上图进行了简化，没有画出NodePort和LoadBalancer。 采用API Gateway和Sidecar Proxy一起作为服务网格的流量入口，既能够通过对网关进行定制开发满足产品对API网关的各种需求，又可以在网络入口处利用服务网格提供的灵活的路由能力和分布式跟踪，策略等管控功能，是服务网格产品入口网关的一个理想方案。 性能方面的考虑：从上图可以看到，采用该方案后，外部请求的处理流程在入口处增加了Sidecar Proxy这一跳，因此该方式会带来少量的性能损失，但该损失是完全可以接受的。 对于请求时延而言，在服务网格中，一个外部请求本来就要经过较多的代理和应用进程的处理，在Ingress处增加一个代理对整体的时延影响基本忽略不计，而且对于绝大多数应用来说，网络转发所占的时间比例本来就很小，99%的耗时都在业务逻辑。如果系统对于增加的该时延非常敏感，则建议重新考虑该系统是否需要采用微服务架构和服务网格。 对于吞吐量而言，如果入口处的网络吞吐量存在瓶颈，则可以通过对API Gateway + Sidecar Proxy组成的Ingress整体进行水平扩展，来对入口流量进行负荷分担，以提高网格入口的网络吞吐量。 参考 本章节参考了下述链接中的文档。 Virtual IPs and Service Proxie - kubernetes.io 如何从外部访问Kubernetes集群中的应用？ - zhaohuabing.com The obstacles to put Istio into production and how we solve them - kubernetes.io Kubernetes NodePort vs LoadBalancer vs Ingress? When should I use what? - medium.com Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/sourcecode/":{"url":"content/sourcecode/","title":"Istio源代码解析","keywords":"","body":"本章节将对Istio各模块的源代码进行分析，以更为深入地理解Istio的机制和实现原理。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 21:45:18 "},"content/sourcecode/service-registry.html":{"url":"content/sourcecode/service-registry.html","title":"服务注册插件机制代码解析","summary":"本文将从代码出发，对Pilot的服务注册插件机制进行分析。","keywords":"","body":"Istio服务注册插件机制 在Istio架构中，Pilot组件负责维护网格中的标准服务模型，该标准服务模型独立于各种底层平台，Pilot通过适配器和各底层平台对接，以使用底层平台中的服务数据填充此标准模型。 例如Pilot中的Kubernetes适配器通过Kubernetes API Server到kubernetes中的Service以及对应的POD实例，将该数据被翻译为标准模型提供给Pilot使用。通过适配器模式，Pilot还可以从Cloud Foundry, Consul中获取服务信息，也可以开发适配器将其他提供服务发现的组件集成到Pilot中。 本文将从代码出发，对Pilot的服务注册机制进行分析。 备注： 本文分析的代码对应Istio commit 58186e1dc3392de842bc2b2c788f993878e0f123 服务注册相关的对象 首先我们来了解一下Pilot中关于服务注册的一些基本概念和相关数据结构。 Istio源码中，和服务注册相关的对象如下面的UML类图所示。 Service 源码文件：pilot/pkg/model/service.go Service用于表示Istio服务网格中的一个服务（例如 catalog.mystore.com:8080)。每一个服务有一个全限定域名(FQDN)和一个或者多个接收客户端请求的监听端口。 一个服务可以有一个可选的 负载均衡器/虚拟IP，DNS解析会对应到该虚拟IP（负载均衡器的IP）上。 一般来说，不管后端的服务实例如何变化，VIP是不会变化的，Istio会维护VIP和后端实例真实IP的对应关系。 例如在Kubernetes中，服务 foo 的FQDN为foo.default.svc.cluster.local， 拥有一个虚拟IP 10.0.1.1，在端口80和8080上监听客户端请求。 type Service struct { // Hostn/服务器名 Hostname Hostname `json:\"hostname\"` // 虚拟IP / 负载均衡器 IP Address string `json:\"address,omitempty\"` // 如果服务部署在多个集群中，ClusterVIPs会保存不同集群中该服务对应的VIP ClusterVIPs map[string]string `json:\"cluster-vips,omitempty\"` // 服务端口列表 Ports PortList `json:\"ports,omitempty\"` // 运行该服务的服务账号 ServiceAccounts []string `json:\"serviceaccounts,omitempty\"` // 该服务是否为一个 “外部服务”， 采用 ServiceEntry 定义的服务该标志为true MeshExternal bool // 服务解析规则： 包括 // ClientSideLB: 由Envoy代理根据其本地的LB pool进行请求路由 // DNSLB: 查询DNS服务器得到IP地址，并将请求发到该IP // Passthrough： 将请求发转发到其原始目的地 Resolution Resolution // 服务创建时间 CreationTime time.Time `json:\"creationTime,omitempty\"` // 服务的一些附加属性 Attributes ServiceAttributes } ServiceInstance 源码文件：pilot/pkg/model/service.go SercieInstance中存放了服务实例相关的信息，一个Service可以对应到一到多个Service Instance，Istio在收到客户端请求时，会根据该Service配置的LB策略和路由规则从可用的Service Instance中选择一个来提供服务。 type ServiceInstance struct { // Endpoint中包括服务实例的IP：Port，UID等 Endpoint NetworkEndpoint `json:\"endpoint,omitempty\"` // 对应的服务 Service *Service `json:\"service,omitempty\"` // 该实例上的标签，例如版本号 Labels Labels `json:\"labels,omitempty\"` // 运行该服务的服务账号 ServiceAccount string `json:\"serviceaccount,omitempty\"` } Registry 源码文件： pilot/pkg/serviceregistry/aggregate/controller.go Registry代表一个通过适配器插入到Pilot中的服务注册表，即Kubernetes，Cloud Foundry 或者 Consul 等具体后端的服务部署/服务注册发现平台。 Registry结构体中包含了Service Registry相关的一些接口和属性。 type Registry struct { // 注册表的类型，例如Kubernetes, Consul, 等等。 Name serviceregistry.ServiceRegistry // 某些类型的服务注册表支持多集群，例如Kubernetes，在这种情况下需要用CluterID来区分同一类型下不同集群的服务注册表 ClusterID string // 控制器，负责向外发送该Registry相关的Service变化消息 model.Controller // 服务发现接口，用于获取注册表中的服务信息 model.ServiceDiscovery } Istio支持以下几种服务注册表类型： 源码文件： pilot/pkg/serviceregistry/platform.go // ServiceRegistry defines underlying platform supporting service registry type ServiceRegistry string const ( // MockRegistry,用于测试的服务注册表，包含两个硬编码的test services MockRegistry ServiceRegistry = \"Mock\" // ConfigRegistry,可以从Configstore中获取定义的service registry，加入到Istio的服务列表中 KubernetesRegistry ServiceRegistry = \"Kubernetes\" // 从Consul获取服务数据的服务注册表 ConsulRegistry ServiceRegistry = \"Consul\" // 采用“Mesh Configuration Protocol”的服务注册表 MCPRegistry ServiceRegistry = \"MCP\" ) 其中支持最完善的就是Kubernetes了，我在项目中使用了Consul，填坑的经验证明对Consul的支持只是原型验证级别的，要在产品中使用的话还需要对其进行较多的改进和优化。 注册表中最后一个类型是 MCP，MCP 是 “Mesh Configuration Protocol\" 的缩写。 Istio 使用了 MCP 实现了一个服务注册和路由配置的标准接口，MCP Server可以从Kubernetes，Cloud Foundry, Consul等获取服务信息和配置数据，并将这些信息通过MCP提供给 MCP Client，即Pilot，通过这种方式，将目前特定平台的相关的代码从Pilot中剥离到独立的MCP服务器中，使Pilot的架构和代码更为清晰。MCP将逐渐替换目前的各种Adapter。更多关于MCP的内容参见： https://docs.google.com/document/d/1o2-V4TLJ8fJACXdlsnxKxDv2Luryo48bAhR8ShxE5-k/edit https://docs.google.com/document/d/1S5ygkxR1alNI8cWGG4O4iV8zp8dA6Oc23zQCvFxr83U/edit Controller 源码文件： pilot/pkg/model/controller.go Controller抽象了一个Service Registry变化通知的接口，该接口会将Service及Service Instance的增加，删除，变化等消息通知给ServiceHandler。 调用Controller的Run方法后，Controller会一直执行，将监控Service Registry的变化，并将通知到注册到Controller中的ServiceHandler中。 type Controller interface { // 添加一个Service Handler，服务的变化会通知到该Handler AppendServiceHandler(f func(*Service, Event)) error // 添加一个Service Instance Handler， 服务实例的变化会通知到该Handler AppendInstanceHandler(f func(*ServiceInstance, Event)) error // 启动Controller的主循环，对Service Catalog的变化进行分发 Run(stop ServiceDiscovery 源码文件： pilot/pkg/model/service.go ServiceDiscovery抽象了一个服务发现的接口，可以通过该接口获取到Service Registry中的Service和Service Instance。 type ServiceDiscovery interface { // 列出该Service Registry中的所有服务 Services() ([]*Service, error) // 根据主机名查询服务 // 该接口已废弃 GetService(hostname Hostname) (*Service, error) // 根据主机名，服务端点和标签查询服务实例 InstancesByPort(hostname Hostname, servicePort int, labels LabelsCollection) ([]*ServiceInstance, error) // 查询边车代理所在节点上的服务实例 GetProxyServiceInstances(*Proxy) ([]*ServiceInstance, error) // 获取边车代理所在的Region,Zone和SubZone GetProxyLocality(*Proxy) string // 管理端口，Istio生成的配置会将管理端口的流量排除，不进行路由处理 ManagementPorts(addr string) PortList // 列出用于监控检查的探针 WorkloadHealthCheckInfo(addr string) ProbeList } Service Registry初始化流程 Service Registry初始化的主要逻辑在Pilot-discovery程序的主函数中，对应的源码为：pilot/cmd/pilot-discovery/main.go和pilot/pkg/bootstrap/server.go。 在pilot/pkg/bootstrap/server.go中，初始化了各种Service Registry，其流程如下图所示： （备注： MCP Registry尚在开发过程中） Pilot将各个Service Registry(Memory, Kube, Consul)保存在serviceregistry.aggreagete.Controller中进行统一管理，Pilot会从所有类型的Registry中查询服务和服务实例，并监控所有Registry的数据变化,当Registry数据变化后，Pilot会清空其内部的缓存并通过ADS接口向Envoy推送更新。 备注：上图中的controller实际上是Service Registry，aggregate controller和具体的各个类型的controller同时实现了Registry要求的controller和discovery interface。 Registry的业务逻辑在Kube Controller和Consul controller中，我们主要使用了Consul Controller， 其主要方法如下： 源码文件： pilot/pkg/serviceregistry/consul/controller.go ▼+Controller : struct [fields] -client : *api.Client -monitor : Monitor [methods] +AppendInstanceHandler(f func(*model.ServiceInstance, model.Event)) : error +AppendServiceHandler(f func(*model.Service, model.Event)) : error +GetIstioServiceAccounts(hostname model.Hostname, ports []int) : []string +GetProxyServiceInstances(node *model.Proxy) : []*model.ServiceInstance, error +GetService(hostname model.Hostname) : *model.Service, error +InstancesByPort(hostname model.Hostname, port int, labels model.LabelsCollection) : []*model.ServiceInstance, error +ManagementPorts(addr string) : model.PortList +Run(stop chan ) +Services() : []*model.Service, error +WorkloadHealthCheckInfo(addr string) : model.ProbeList -getCatalogService(name string, q *api.QueryOptions) : []*api.CatalogService, error -getServices() : map[string][]string, error [functions] +NewController(addr string, interval time.Duration) : *Controller, error 可以看到Consul Controller对象同时实现了Registry要求的Controller和ServiceDiscovery接口，可以提供Registry的变化通知和服务查询相关功能。 目前Consul Controller的实现比较简单粗暴，定时通过Consul的Rest API获取服务数据并和上一次的查询结果进行对比，如果数据发生了变化则通知Pilot discovery进行更新。该方式发起了大量对Consul Server的HTTP请求，会导致Consul Server CPU占用率高和大量TCP Socket处于TIME_WAIT状态，不能直接在产品环境下使用。 源码文件： pilot/pkg/serviceregistry/consul/monitor.go //定时轮询Consul Server Rest接口，以获取服务数据变化 func (m *consulMonitor) run(stop 我们在Consul Registry中增加了缓存，并降低了Pilot轮询Consul server的频率，以减少Pilot频繁调用给Consul server带来的大量压力，下一步打算采用Consul watch来代替轮询，优化Consul Registry的服务变化通知机制。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 22:02:44 "},"content/sourcecode/envoy-proxy.html":{"url":"content/sourcecode/envoy-proxy.html","title":"Envoy Proxy代码构建分析","summary":"Istio由控制面和数据面组成。其中Envoy是Istio在数据面缺省使用的转发代理，Istio利用Envoy的四层和七层代理功能对网格中微服务之间的调用流量进行转发。今天我>们来分析一下Istio 使用到的Envoy构建流程。","keywords":"","body":"Istio由控制面和数据面组成。其中Envoy是Istio在数据面缺省使用的转发代理，Istio利用Envoy的四层和七层代理功能对网格中微服务之间的调用流量进行转发。今天我们来分析一下Istio 使用到的Envoy构建流程。 https://github.com/istio/proxy 这个库中包含了Istio对Envoy的扩展，包括用于对接Mixer的Filter和安全认证的Filter。但这个库中并不包含Envoy自身的源代码，因此这个库在构建时会从Github上下载Envoy源码进行联合编译。 编译工具 Envoy采用了Bazel进行构建。 Bazel是一种高层构建语言，类似Make，Maven和Gradle。其特点是可读性较好，支持跨语言，跨平台编译；并且可以定义代码库之间的依赖关系，支持跨代码库的联合构建。Bazel定义构建的依赖关系和规则，并管理构建生成的临时文件及二进制文件，具体的编译工作是调用各个语言的编译工具如GCC, JAVAC等完成的。 为了理解Envoy的编译过程，我们需要先了解Bazel的几个基础概念 workspace: 文件系统中的一个目录，该目录中包含了用于编译软件所需的所有源文件。每个工作空间中有一个WORKSPACE文件，该文件用于描述该工作空间的外部依赖，例如依赖的Github上的第三方代码。 Package: 是一组用于相关文件的集合，该目录中包含一个BUILD文件，此文件中描述了该程序包的构建方式。 target: 生成的目标，一般是一个lib或者二进制文件。 target是一个构建规则(build rule)的实例，一般包含构建所需的源文件，构建目标的名称。rule还可以嵌套，一个rule的输出文件可以作为另一个rule的输入文件。例如一个二进制文件编译的target可以依赖另一个target生成的lib。另外target还可以依赖外部Repository中的另一个target，这个外部Repository可以是文件系统上另一个文件夹下的项目，github的项目或者http下载的代码。外部Repository在WORKSPACE文件中进行定义。 编译Envoy 首先参考Bazel的官方文档安装Bazel，并且需要安装gcc等相关工具。 设置gcc及g++环境变量 export CC=/usr/bin/gcc-5; export CXX=/usr/bin/g++-5 下载源码并进行构建 git clone https://github.com/istio/proxy.git cd proxy make build_envoy 如果出现错误提示，一般是由于编译所需的软件未安装导致，请根据提示信息进行安装。 如果一切顺利，bazel会在proxy目录下创建一个目录链接bazel-bin，指向生成的二进制文件。 编译过程分析 源码目录结构如下，主要的构建逻辑在引号包含的文件中。 ├── \"BUILD\" ├── \"Makefile\" ├── \"WORKSPACE\" ├── src │ ├── envoy -- envoy filter 插件源码 │ │ ├── alts │ │ │ ├── *.cc │ │ │ ├── *.h │ │ │ └── \"BUILD\" │ │ ├── \"BUILD\" │ │ ├── http │ │ │ ├── authn --认证 filte │ │ │ │ ├── *.cc │ │ │ │ ├── *.h │ │ │ │ └── \"BUILD\" │ │ │ ├── jwt_auth --jwt 认证 filter │ │ │ │ ├── *.cc │ │ │ │ ├── *.h │ │ │ │ └── \"BUILD\" │ │ │ └── mixer --mixer filter，实现metrics上报，Quota(Rate Limiting (处理http协议) │ │ │ ├── *.cc │ │ │ ├── *.h │ │ │ └── \"BUILD\" │ │ ├── tcp │ │ │ └── mixer --mixer filter(处理tcp协议) │ │ │ ├── *.cc │ │ │ ├── *.h │ │ │ └── \"BUILD\" │ │ └── utils │ │ ├── *.cc │ │ ├── *.h │ │ └── \"BUILD\" │ └── istio │ └── ** ├── test │ └── ** └── tools └── ** 编译的入口是根目录下的Makefile文件 # Build only envoy - fast build_envoy: CC=$(CC) CXX=$(CXX) bazel $(BAZEL_STARTUP_ARGS) build $(BAZEL_BUILD_ARGS) //src/envoy:envoy @bazel shutdown 从中可以看到，调用了bazel进行构建，其构建的target为 //src/envoy:envoy 。这是bazel的语法，表明调用src/envoy这个目录下BUILD文件中Envoy这个target。 打开src/BUILD文件，查看该target的内容 envoy_cc_binary( name = \"envoy\", repository = \"@envoy\", visibility = [\"//visibility:public\"], deps = [ \"//src/envoy/http/authn:filter_lib\", \"//src/envoy/http/jwt_auth:http_filter_factory\", \"//src/envoy/http/mixer:filter_lib\", \"//src/envoy/tcp/mixer:filter_lib\", \"//src/envoy/alts:alts_socket_factory\", \"@envoy//source/exe:envoy_main_entry_lib\", ], ) cc_binary表明该target对应的是c++二进制rule，其中deps部分是其依赖的其他target。前5个target都是本地依赖，对应到源码目录中的其他子目录下的BUILD文件，其中最后一个比较特殊，是一个外部依赖，该外部库为envoy。 外部库定义在根目录下的WORKSPACE文件中。 ENVOY_SHA = \"de039269f54aa21aa0da21da89a5075aa3db3bb9\" http_archive( name = \"envoy\", strip_prefix = \"envoy-\" + ENVOY_SHA, url = \"https://github.com/envoyproxy/envoy/archive/\" + ENVOY_SHA + \".zip\", ) 该文件通过http_archive定义了一个外部repository，bazel在执行//src/envoy:envoy这个target时，发现该target依赖这个外部repository，根据http_archive中的描述，从指定的url下载该依赖的源码，并进行编译。 编译过程中的依赖关系如下图所示： Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 22:15:46 "},"content/sourcecode/sidecar-injection.html":{"url":"content/sourcecode/sidecar-injection.html","title":"Sidecar自动注入原理","summary":"Kubernets 1.9版本引入了Admission Webhook(web 回调)扩展机制，通过Webhook,开发者可以非常灵活地对Kubernets API Server的功能进行扩展，在API Server创建资源时对资源进行验证或者修改。 Istio 0.7版本就利用了Kubernets webhook实现了sidecar的自动注入。","keywords":"","body":"前言 Kubernets 1.9版本引入了Admission Webhook(web 回调)扩展机制，通过Webhook,开发者可以非常灵活地对Kubernets API Server的功能进行扩展，在API Server创建资源时对资源进行验证或者修改。 使用webhook的优势是不需要对API Server的源码进行修改和重新编译就可以扩展其功能。插入的逻辑实现为一个独立的web进程，通过参数方式传入到kubernets中，由kubernets在进行自身逻辑处理时对扩展逻辑进行回调。 Istio 0.7版本就利用了Kubernets webhook实现了sidecar的自动注入。 什么是Admission Admission是Kubernets中的一个术语，指的是Kubernets API Server资源请求过程中的一个阶段。如下图所示，在API Server接收到资源创建请求时，首先会对请求进行认证和鉴权，然后经过Admission处理，最后再保存到etcd。 从图中看到，Admission中有两个重要的阶段，Mutation和Validation，这两个阶段中执行的逻辑如下： Mutation Mutation是英文“突变”的意思,从字面上可以知道在Mutation阶段可以对请求内容进行修改。 Validation 在Validation阶段不允许修改请求内容，但可以根据请求的内容判断是继续执行该请求还是拒绝该请求。 Admission webhook 通过Admission webhook,可以加入Mutation和Validation两种类型的webhook插件，这些插件和Kubernets提供的预编译的Admission插件具有相同的能力。可以想到的用途包括： 修改资源。例如Istio就通过Admin Webhook在Pod资源中增加了Envoy sidecar容器。 自定义校验逻辑，例如对资源名称有一些特殊要求。或者对自定义资源的合法性进行校验。 采用Webhook自动注入Istio Sidecar Kubernets版本要求 webhook支持需要Kubernets1.9或者更高的版本,使用下面的命令确认kube-apiserver的Admin webhook功能已启用。 kubectl api-versions | grep admissionregistration admissionregistration.k8s.io/v1beta1 生成sidecar injection webhook的密钥和证书 Webhook使用数字证书向kube-apiserver进行身份认证，因此需要先使用工具生成密钥对，并向Istio CA申请数字证书。 ./install/kubernetes/webhook-create-signed-cert.sh / --service istio-sidecar-injector / --namespace istio-system / --secret sidecar-injector-certs 将生成的数字证书配置到webhook中 cat install/kubernetes/istio-sidecar-injector.yaml | / ./install/kubernetes/webhook-patch-ca-bundle.sh > / install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml 创建sidecar injection configmap kubectl apply -f install/kubernetes/istio-sidecar-injector-configmap-release.yaml 部署sidecar injection webhook kubectl apply -f install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml 通过命令查看部署好的webhook injector kubectl -n istio-system get deployment -listio=sidecar-injector Copy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE istio-sidecar-injector 1 1 1 1 1d 开启需要自动注入sidecar的namespace kubectl label namespace default istio-injection=enabled kubectl get namespace -L istio-injection NAME STATUS AGE ISTIO-INJECTION default Active 1h enabled istio-system Active 1h kube-public Active 1h kube-system Active 1h 参考 Extensible Admission is Beta Installing the Istio Sidecar Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 22:13:49 "},"content/debug-istio/":{"url":"content/debug-istio/","title":"Istio故障定位方法","keywords":"","body":"服务网格为微服务提供了一个服务通信的基础设施层，统一为上层的微服务提供了服务发现，负载均衡，重试，断路等基础通信功能，以及服务路由，灰度发布，chaos测试等高级管控功能。 服务网格的引入大大降低了个微服务应用的开发难度，让微服务应用开发人员不再需要花费大量时间用于保障底层通讯的正确性上，而是重点关注于产生用户价值的业务需求。 然而由于微服务架构的分布式架构带来的复杂度并未从系统中消失，而是从各个微服务应用中转移到了服务网格中。由服务网格对所有微服务应用的通讯进行统一控制，好处是可以保证整个系统中分布式通讯策略的一致性，并可以方便地进行集中管控。 除微服务之间分布式调用的复杂度之外，服务网格在底层通讯和微服务应用之间引入了新的抽象层，为系统引入了一些额外的复杂度。在此情况下，如果服务网格自身出现故障，将对上层的微服务应用带来灾难性的影响。 当系统中各微服务应用之间的通讯出现异常时，我们可以通过服务网格提供的分布式调用跟踪，故障注入，服务路由等手段快速进行分析和处理。但如果服务网格系统自身出现问题的话，我们如何才能快速进行分析处理呢？ Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/debug-istio/服务网格中的请求转发流程.html":{"url":"content/debug-istio/服务网格中的请求转发流程.html","title":"请求转发流程","keywords":"","body":"微服务之间的流量经过服务网格接管后，在微服务之间引入了多个代理层，微服务之间的通信变得更为复杂了。下图分析了一个客户端请求的流量是如何在服务网格中进行路由的。 备注：由于Istio Ingress Gateway的功能缺少API管理功能，因此下图采用了API Gateway + sidecar来作为Ingress，和原始的Istio Ingress有所不同，但流量转发逻辑类似。Istio Ingress和API Gateway的差异分析参见文章：https://zhaohuabing.com/post/2018-12-27-the-obstacles-to-put-istio-into-production/#service-mesh-and-api-gateway 从上图可以看到，客户端请求从进入系统入口的Ingress，到到达后端提供服务的应用，经过了多次IPtable的重定向和Envoy的转发。 其转发流程如下：（备注：为简略起见，本流程中未描述客户端端口） 1: Client IP ---> Ingress Public IP: Server Port 2: Ingress Internal IP ---> Service 1 IP: Server Port 2.1: Ingress Internal IP ----(IPtable DNAT)---> 127.0.0.1: 15001 2.2: Ingress Internal IP---> Service1 IP:Server Port 2.2.1: Ingress Internal IP ----(IPtable DNAT)---> 127.0.0.1: 15001 2.2.2: Service1 Sidecar IP ---> 127.0.0.1: Service1 Server Port 3: Service1 IP ---> Service2 Server Port 3.1: Service1 IP ----(IPtable DNAT)---> 127.0.0.1: 15001 3.2: Service1 IP ----> Service2 Server Port ...... 如果Istio配置错误导致通信故障，从应用层面上很难直接查找原因。需要通过各种手段从TCP通讯层，Pilot,Envoy等多处获取信息，对故障进行分析。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/debug-istio/Pilot调试信息.html":{"url":"content/debug-istio/Pilot调试信息.html","title":"Pilot调试信息","keywords":"","body":"Pilot提供了一个调试端口9093，可以通过向调试端口发送REST请求来分析和查看标准数据面接口(Envoy xDS API)的数据和Pilot内部存储的状态信息。 xDS接口相关调试信息 发送给Enovy的Listener，Filter及Route配置 curl http://127.0.0.1:9093/debug/adsz 各个Cluster中配置的Endpoint curl http://127.0.0.1:9093/debug/edsz Cluster信息 curl http://127.0.0.1:9093/debug/cdsz 备注：上述接口中的配置信息在Envoy第一次连接到Pilot中时才会生成，在此之前，通过接口无法获取到数据。 Pilot内部的配置信息 服务注册表信息 curl http://127.0.0.1:9093/debug/registryz 所有的endpoint curl http://127.0.0.1:9093/debug/endpointz[?brief=1] 所有的配置信息 curl http://127.0.0.1:9093/debug/configz Pilot自身的一些性能数据 curl http://127.0.0.1:9093/metrics 参考：https://github.com/istio/istio/tree/master/pilot/pkg/proxy/envoy/v2 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/debug-istio/Envoy调试信息.html":{"url":"content/debug-istio/Envoy调试信息.html","title":"Envoy调试信息","keywords":"","body":"查看Envoy配置 Envoy在localhost:15000上提供了Admin端口，可以通过docker exec命令获取其Pilot向其下发的配置信息。 sudo docker exec a83696c9e7a2 curl http://127.0.0.1:15000/config_dump 除此以外，Envoy还提供了其他调试信息，可以通过help进行查询 sudo docker exec a83696c9e7a2 curl http://127.0.0.1:15000/help admin commands are: /: Admin home page /certs: print certs on machine /clusters: upstream cluster status /config_dump: dump current Envoy configs (experimental) /cpuprofiler: enable/disable the CPU profiler /healthcheck/fail: cause the server to fail health checks /healthcheck/ok: cause the server to pass health checks /help: print out list of admin commands /hot_restart_version: print the hot restart compatibility version /listeners: print listener addresses /logging: query/change logging levels /quitquitquit: exit the server /reset_counters: reset all counters to zero /runtime: print runtime values /runtime_modify: modify runtime values /server_info: print server version/status information /stats: print server stats /stats/prometheus: print server stats in prometheus format 参考： https://www.envoyproxy.io/docs/envoy/latest/operations/admin IPtable规则 proxy_init 容器会将设置的IPtable内容输出到标准输出中，可以查看到对那些IP端进行了拦截。 sudo docker logs 3ad9 ......忽略掉前面无关的内容...... # Generated by iptables-save v1.6.0 on Fri Jan 11 07:10:19 2019 *nat :PREROUTING ACCEPT [0:0] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :POSTROUTING ACCEPT [0:0] :ISTIO_OUTPUT - [0:0] :ISTIO_REDIRECT - [0:0] -A PREROUTING -m comment --comment \"istio/install-istio-prerouting\" -j ISTIO_REDIRECT -A OUTPUT -p tcp -m comment --comment \"istio/install-istio-output\" -j ISTIO_OUTPUT -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m comment --comment \"istio/redirect-implicit-loopback\" -j ISTIO_REDIRECT -A ISTIO_OUTPUT -m owner --uid-owner 1337 -m comment --comment \"istio/bypass-envoy\" -j RETURN -A ISTIO_OUTPUT -d 127.0.0.1/32 -m comment --comment \"istio/bypass-explicit-loopback\" -j RETURN -A ISTIO_OUTPUT -d 172.168.40.4/32 -m comment --comment \"istio/bypass-msb-ip\" -j RETURN -A ISTIO_OUTPUT -d 100.100.0.0/16 -m comment --comment \"istio/redirect-ip-range-100.100.0.0/16\" -j ISTIO_REDIRECT -A ISTIO_OUTPUT -d 172.168.40.0/24 -m comment --comment \"istio/redirect-ip-range-172.168.40.0/24\" -j ISTIO_REDIRECT -A ISTIO_OUTPUT -m comment --comment \"istio/bypass-default-outbound\" -j RETURN -A ISTIO_REDIRECT -p tcp -m comment --comment \"istio/redirect-to-envoy-port\" -j REDIRECT --to-ports 15001 COMMIT # Completed on Fri Jan 11 07:10:19 2019 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/debug-istio/Consul调试信息.html":{"url":"content/debug-istio/Consul调试信息.html","title":"Consul调试信息","keywords":"","body":"如果采用了Consul作为Service Registry，可以通过下面的接口查看Consul中的服务注册信息，以和Pilot及Envoy中的服务信息进行对比分析。 查看Consul中注册的所有服务 curl http://172.167.40.2:1107/v1/catalog/services 查看某一个服务的具体内容 curl http://172.167.40.2:1107/v1/catalog/service/{service} 参考： https://www.consul.io/api/catalog.html Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "},"content/debug-istio/协议层调试信息.html":{"url":"content/debug-istio/协议层调试信息.html","title":"协议层调试信息","keywords":"","body":"Strace是一个linux系统下的小工具，可以用于跟踪一个指定进程的系统调用和其接收到的数据。该工具可以查看写入/收到file descriptor的内容，可以被用于分析微服务间的HTTP通信。 首先通过ps找到需要跟踪的进程。 sudo ps -ef|grep istio-demo 然后使用strace来监控该进程的网络消息。 sudo strace -p 91558 -f -e trace=network -s 1000 下面是strace对一个HTTP请求的相关调试输出信息，可以看到该进程对外发出了一个http请求，并收到了一个404错误。可以从输出中查看到HTTP请求的完整内容，包括Method, URL, Header等内容。 [pid 93648] sendto(285, \"GET /api/istioserver/v1/animals/panda HTTP/1.1\\r\\nHost: 100.100.0.112:9090\\r\\nConnection: Keep-Alive\\r\\nAccept-Encoding: gzip\\r\\nUser-Agent: okhttp/3.3.0\\r\\n\\r\\n\", 149, 0, NULL, 0) = 149 [pid 93648] recvfrom(285, \"HTTP/1.1 404 Not Found\\r\\ndate: Fri, 11 Jan 2019 04:59:07 GMT\\r\\nserver: envoy\\r\\ncontent-length: 0\\r\\n\\r\\n\", 8192, MSG_DONTWAI strace的使用方法可参考： http://man7.org/linux/man-pages/man1/strace.1.html Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:45:32 "}}